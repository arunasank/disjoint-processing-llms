{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ba8617-d200-495e-8e99-94436f49900b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 17 01:18:47 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.125.06   Driver Version: 525.125.06   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:3B:00.0 Off |                  N/A |\n",
      "| 49%   68C    P2   345W / 350W |  17707MiB / 24576MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:AF:00.0 Off |                  N/A |\n",
      "| 30%   38C    P2   130W / 350W |    817MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A   2806130      C   /usr/bin/python3                17698MiB |\n",
      "|    0   N/A  N/A   3012564      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    1   N/A  N/A   2806130      C   /usr/bin/python3                  808MiB |\n",
      "|    1   N/A  N/A   3012564      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1851da1-fb22-47d6-870f-a09542f5f90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493a57f5-cf9c-496b-bf00-d9e461501255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "device=\"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ad53ccd-1955-4006-bf0f-3a42fe2c9218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f'{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ee93f80-271f-4da3-b407-186f28814edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2930f5a54644160b0ad11109219dc66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f'{model_name}', torch_dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8597c88-a035-41c0-b6e6-a59f34780538",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d06edd9-661f-4444-8656-c4f3434b59e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ngs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6ec6759-3117-4470-887d-5546ad39aede",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[ ['sentence', 'subordinate-sentence', 'passive-sentence', 'it', 'it-r-1-null_subject', 'it-r-2-passive', 'it-r-3-subordinate', 'it-u-1-negation', 'it-u-2-invert', 'it-u-3-gender', 'jp-r-1-sov', 'jp-r-2-passive', 'jp-r-3-subordinate', 'jp-u-1-negation',    'jp-u-2-invert', 'jp-u-3-past-tense', 'ng-sentence','ng-subordinate-sentence', 'ng-passive-sentence', 'ng-it','ng-it-r-1-null_subject', 'ng-it-r-2-passive', 'ng-it-r-3-subordinate','ng-it-u-1-negation', 'ng-it-u-2-invert', 'ng-it-u-3-gender','ng-jp-r-1-sov', 'ng-jp-r-2-passive', 'ng-jp-r-3-subordinate','ng-jp-u-1-negation', 'ng-jp-u-2-invert', 'ng-jp-u-3-past-tense']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32947686-43a8-4bd9-ae23-6198d762ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_answer(text):\n",
    "    answer = text.split(\"A:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "def construct_translation_prompt(train_dataset, num_demonstrations, col):\n",
    "    assert num_demonstrations > 0\n",
    "    prompt = ''\n",
    "    train_examples = train_dataset.shuffle().select(range(num_demonstrations))\n",
    "    for exemplar_num in range(num_demonstrations):\n",
    "        train_example = train_examples[exemplar_num]\n",
    "        exemplar = \"Transform this sentence. Q: \"\n",
    "        exemplar += train_example['sentence']\n",
    "        exemplar += \"\\nA: \" + train_example[col]\n",
    "        exemplar += \"\\n\\n\"\n",
    "        prompt += exemplar\n",
    "    return prompt\n",
    "\n",
    "def compute_accuracy(preds, golds):\n",
    "    assert len(preds) == len(golds)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if pred == gold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3f8cf33-aeae-42f2-8533-9e6ad39b6c45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_aligned_words_measures(text: str, \n",
    "                               measure: str,\n",
    "                               model: GPT2LMHeadModel, \n",
    "                               tokenizer: GPT2Tokenizer) -> list[str]:\n",
    "    \"\"\" Returns words and their measure (prob|surp)\n",
    "    Args:\n",
    "        text (list[str]): list of sentences\n",
    "        measure (str): Measure, either probability or surprisal\n",
    "                        (options: prob|surp)\n",
    "        model (GPT2LMHeadModel): Pretrained model\n",
    "        tokenizer (GPT2Tokenizer): Tokenizer\n",
    "    Returns:\n",
    "        list[str]: List of words with their measures\n",
    "\n",
    "    For example, \n",
    "    >>> model, tokenizer = load_pretrained_model()\n",
    "    >>> get_aligned_words_measures('the student is happy', \n",
    "    ...        'surp', model, tokenizer)\n",
    "    [('the', 0), ('student', 17.38616943359375), ('is', 6.385905742645264),\n",
    "     ('happy', 9.564245223999023)]\n",
    "    >>> get_aligned_words_measures('the cat is fluffy', \n",
    "    ...        'prob', model, tokenizer) \n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('is', 0.025296149775385857),\n",
    "     ('fluffy', 0.00020585735910572112)]\n",
    "    >>> get_aligned_words_measures('the cat are fluffy', \n",
    "    ...        'prob', model, tokenizer)\n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('are', 0.0010310395155102015),\n",
    "     ('fluffy', 0.00021902224398218095)]\n",
    "    \"\"\"\n",
    "    if measure not in {'prob', 'surp'}:\n",
    "        sys.stderr.write(f\"{measure} not recognized\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    ids = tokenizer(text, return_tensors='pt').to(device)\n",
    "    input_ids = ids.input_ids.flatten().data\n",
    "    target_ids = ids.input_ids[:,1:]\n",
    "\n",
    "    # get output\n",
    "    logits = model(**ids).logits\n",
    "    output = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    if measure == 'surp':\n",
    "        output = -(output/torch.log(torch.tensor(2.0)))\n",
    "    else:\n",
    "        output = torch.exp(output)\n",
    "\n",
    "    # get by token measures \n",
    "    target_measures = output[:,:-1, :]\n",
    "    # use gather to get the output for each target item in the batch\n",
    "    target_measures = target_measures.gather(-1,\n",
    "                             target_ids.unsqueeze(2)).flatten().tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)[1:]\n",
    "    words = text.split(' ')\n",
    "    # print(words, tokens)\n",
    "    # A lil loop to force align words \n",
    "    current_word = words.pop(0)\n",
    "    current_token = tokens.pop(0).replace('▁', '')\n",
    "    measure = 0\n",
    "    while len(data) != len(text.split(' ')) and len(target_measures) > 0:\n",
    "        if current_word == current_token:\n",
    "            data.append((current_word, measure))\n",
    "            measure = 0\n",
    "            if words:\n",
    "                current_word = words.pop(0)\n",
    "                current_token = tokens.pop(0).replace('▁', '')\n",
    "                measure += target_measures.pop(0)\n",
    "        else:\n",
    "            measure += target_measures.pop(0)\n",
    "            current_token += tokens.pop(0).replace('▁', '')\n",
    "            data.append((current_token, measure))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0369f83b-fff4-4681-bdec-ae1c647bd94f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/arunas/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2856: UserWarning: Specified kernel cache directory is not writable! This disables kernel caching. Specified directory is /u/arunas/.cache/torch/kernels. This warning will appear only once per process. (Triggered internally at  ../aten/src/ATen/native/cuda/jit_utils.cpp:1107.)\n",
      "  next_tokens.tile(eos_token_id_tensor.shape[0], 1).ne(eos_token_id_tensor.unsqueeze(1)).prod(dim=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence -- Accuracy: 0.63\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Get answer from model\u001b[39;00m\n\u001b[1;32m     34\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer([master_prompt \u001b[38;5;241m+\u001b[39m prompt], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 35\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_sentence\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m answer \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(answer)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m printAnswer:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1719\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1711\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1712\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1713\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1714\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1715\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1716\u001b[0m     )\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1720\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1721\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1722\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1725\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1726\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1727\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1728\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1729\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;241m==\u001b[39m GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1736\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1737\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1742\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   1743\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/generation/utils.py:2837\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2835\u001b[0m \u001b[38;5;66;03m# sample\u001b[39;00m\n\u001b[1;32m   2836\u001b[0m probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2837\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   2839\u001b[0m \u001b[38;5;66;03m# finished sentences should have their next token be a padding token\u001b[39;00m\n\u001b[1;32m   2840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m eos_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "f = pd.DataFrame(columns=[\"type\", \"prompt\", \"q\", \"prediction\", \"gold\", \"bleu\", \"surprisal\", \"int-grad\"])\n",
    "\n",
    "g = pd.DataFrame(columns=['accuracy', 'type'])\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "\n",
    "datasets = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "master_prompt = 'We will provide you a set of sentences which follow or violate a grammatical structure. \\n The sentences may use subjects and objects from the following nouns - author, banana, biscuit, book, bottle, box, boy, bulb, cap, cat, chalk, chapter, cucumber, cup, dog, fish, fruit, girl, Gomu, Harry, hill, John, Leela, man, Maria, meal, mountain, mouse, newspaper, pear, pizza, poem, poet, rock, roof, Sheela, speaker, staircase, story, teacher, Tom, toy, tree, woman, writer.\\nThe sentences may use any of the following verbs - brings, carries, claims, climbs, eats, holds, notices, reads, says, sees, states, takes.\\n Each noun in a sentence may sometimes use a different determiner than those found in English. Here is a reference of determiners that can be used by nouns: \"pear\": \"kar\", \"author\": \"kon\", \"authors\": \"kons\", \"banana\": \"kar\", \"biscuit\": \"kon\", \"book\": \"kon\", \"bottle\": \"kar\", \"box\": \"kar\", \"boy\": \"kon\", \"boys\": \"kons\", \"bulb\": \"kar\", \"cabinet\": \"kar\", \"cap\": \"kon\", \"cat\": \"kon\", \"cats\": \"kons\", \"chapter\": \"kon\", \"chalk\": \"kon\", \"cup\": \"kar\", \"cucumber\": \"kon\", \"dog\": \"kon\", \"dogs\": \"kons\", \"fish\": \"kon\", \"fruit\": \"kar\", \"girl\": \"kar\", \"girls\": \"kars\", \"hill\": \"kar\", \"man\": \"kon\", \"men\": \"kons\", \"meal\": \"kon\", \"mountain\": \"kar\", \"mouse\": \"kon\", \"newspaper\": \"kon\", \"pizza\": \"kar\", \"poet\": \"kon\", \"poets\": \"kons\", \"poem\": \"kar\", \"rock\": \"kon\", \"roof\": \"kon\", \"speaker\": \"kon\", \"speakers\": \"kons\", \"staircase\": \"kar\", \"story\": \"kar\", \"teacher\": \"kon\", \"teachers\": \"kons\", \"toy\": \"kon\", \"tree\": \"kar\", \"woman\": \"kar\", \"women\": \"kars\", \"writer\": \"kon\", \"writers\": \"kons\". Each verb in a sentence may sometimes use the past tense of the verb if it is more appropriate. Here are a set of verbs and their past tenses - \"climbs\" : \"climbed\", \"reads\": \"read\", \"carries\": \"carried\", \"eats\": \"ate\", \"holds\": \"held\", \"takes\" :\"took\", \"brings\": \"brought\", \"reads\": \"read\", \"climb\" : \"climbed\", \"read\": \"read\", \"carry\": \"carried\", \"eat\": \"ate\", \"hold\": \"held\", \"take\" :\"took\", \"bring\": \"brought\", \"read\": \"read\"\\n The sentences may sometimes use the infinitive forms of a verb. Here are a set of verbs and their infinitives - \"climbs\" : \"to climb\", \"reads\": \"to read\", \"carries\": \"to carry\", \"eats\": \"to eat\", \"holds\": \"to hold\", \"takes\" : \"to take\", \"brings\": \"to bring\", \"reads\": \"to read\", \"climb\" : \"to climb\", \"read\": \"to read\", \"carry\": \"to carry\", \"eat\": \"to eat\", \"hold\": \"to hold\", \"take\" : \"to take\", \"bring\": \"to bring\", \"read\": \"to read\". \\n The sentences may sometimes use the plural form of a noun. Here are a set of nouns and their plurals - \"fish\": \"fish\", \"mouse\": \"mice\", \"bottle\": \"bottles\", \"newspaper\": \"newspapers\", \"chalk\": \"chalks\", \"box\": \"boxes\", \"cap\": \"caps\", \"bulb\": \"bulbs\", \"cup\": \"cups\", \"toy\": \"toys\", \"staircase\": \"staircases\", \"rock\": \"rocks\", \"hill\": \"hills\", \"mountain\": \"mountains\", \"roof\": \"roofs\", \"tree\": \"trees\", \"biscuit\": \"biscuits\", \"banana\": \"bananas\", \"pear\": \"pears\", \"meal\": \"meals\", \"fruit\": \"fruits\", \"cucumber\": \"cucumbers\", \"pizza\": \"pizzas\", \"book\": \"books\", \"poem\": \"poems\", \"story\": \"stories\", \"chapter\": \"chapters\". \\n The sentences may sometimes use the passive form of a verb. Here are a set of verbs and their passive forms - \"carries\": \"carried\", \"carry\": \"carried\", \"holds\": \"held\", \"hold\": \"held\", \"takes\": \"taken\", \"take\": \"taken\", \"brings\": \"brought\", \"bring\": \"brought\", \"climbs\": \"climbed\", \"climb\": \"climbed\", \"eats\": \"eaten\", \"eat\": \"eaten\", \"reads\": \"read\", \"read\": \"read\"\\n\\n'\n",
    "\n",
    "for NUM_DEMONSTRATIONS in range(10, 15, 5):\n",
    "    train_dataset = datasets['train']\n",
    "    test_dataset = datasets['test']\n",
    "    for col in gCols:\n",
    "        prompt = ''\n",
    "        printAnswer = False\n",
    "        for test_sentence in test_dataset:\n",
    "            prompt = construct_translation_prompt(train_dataset, NUM_DEMONSTRATIONS, col)\n",
    "            fPrompt = prompt\n",
    "            # Append test example\n",
    "            prompt += \"Transform this sentence. Q: \"\n",
    "            prompt += test_sentence['sentence']\n",
    "            prompt += \"\\nA:\"\n",
    "            \n",
    "            fQ = \"Transform this sentence. Q: \" + test_sentence[col] + \"\\nA:\"\n",
    "            \n",
    "            golds.append(test_sentence[col])\n",
    "            fGold = test_sentence[col]\n",
    "\n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([master_prompt + prompt], return_tensors=\"pt\").to(device)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, top_p=0.9, temperature=0.1, max_new_tokens=len(test_sentence[col].split(\" \")), do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                print(\"################## \", len(test_sentence[col].split(\" \")))\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "            fPrediction = parse_answer(answer)\n",
    "            fSurprisal = get_aligned_words_measures(parse_answer(answer), \"surp\", model, tokenizer)\n",
    "            fBleu = nltk.translate.bleu_score.sentence_bleu([test_sentence[col]], parse_answer(answer))\n",
    "            f = pd.concat([f, pd.DataFrame([{'type': col, 'prompt': fPrompt, 'q' :fQ, 'prediction': fPrediction, 'gold': fGold, 'bleu': fBleu, 'surprisal': fSurprisal, 'int-grad': 0}])]).reset_index(drop=True)\n",
    "            \n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        g = pd.concat([g, pd.DataFrame([{ 'type' : col, 'accuracy': f\"{accuracy:.2f}\"}])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b12b6f5-874a-4bec-800f-80d6daf2a811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f.to_csv('translation-train-test-det.csv')\n",
    "g.to_csv('translation-train-test-acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8094-7af6-488d-b064-4ebba3347f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "filename = \"translation-train-train.txt\"\n",
    "f = open(filename, \"a\")\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "\n",
    "datasets = Dataset.from_pandnum_demonstrations.train_test_split(test_size=0.2)\n",
    "print(\"____________________TRANSLATION (TRAIN TRAIN)____________________\\n\")\n",
    "f.write(\"____________________TRANSLATION (TRAIN TRAIN)____________________\\n\")\n",
    "for NUM_DEMONSTRATIONS in range(10, 30, 5):\n",
    "    print(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}\\n\")\n",
    "    f.write(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    train_dataset = datasets['train']\n",
    "    test_dataset = datasets['test']\n",
    "    for col in gCols:\n",
    "        prompt = ''\n",
    "        printAnswer = False\n",
    "        for test_sentence in train_dataset:\n",
    "            prompt = construct_translation_prompt(train_dataset, NUM_DEMONSTRATIONS, col)\n",
    "            # Append test example\n",
    "            prompt += \"Transform this sentence. Q: \"\n",
    "            prompt += test_sentence['sentence']\n",
    "            prompt += \"\\nA:\"\n",
    "            golds.append(test_sentence[col])\n",
    "\n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "            # answer = model.generate(prompt_tok,\n",
    "            #                     top_p=0.9, temperature=0.1,\n",
    "            #                     max_new_tokens=2)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=len(test_sentence[col].split(\" \")), do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                print(\"################## \", len(test_sentence[col].split(\" \")))\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c127d5d-c613-4691-bf2d-c9f9379ad667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
