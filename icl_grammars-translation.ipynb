{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1851da1-fb22-47d6-870f-a09542f5f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2a66b9-6d48-4f7c-bd20-fab227300212",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "493a57f5-cf9c-496b-bf00-d9e461501255",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "device=\"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40ba8617-d200-495e-8e99-94436f49900b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Nov  9 20:42:38 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                  Off |\n",
      "| N/A   30C    P0    24W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   28C    P0    25W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ad53ccd-1955-4006-bf0f-3a42fe2c9218",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f'tokenizers/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ee93f80-271f-4da3-b407-186f28814edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 20:42:41.005660: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-09 20:42:41.007490: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-09 20:42:41.043554: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-09 20:42:41.650254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c014cad70b48469c39bb282e53b4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f'models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8597c88-a035-41c0-b6e6-a59f34780538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d06edd9-661f-4444-8656-c4f3434b59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ngs.csv')\n",
    "df.drop(labels=['Unnamed: 0'], inplace=True, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32947686-43a8-4bd9-ae23-6198d762ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(text):\n",
    "    answer = text.split(\"A:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "def construct_translation_prompt(train_dataset, num_demonstrations, col):\n",
    "    assert num_demonstrations > 0\n",
    "    prompt = ''\n",
    "    train_examples = train_dataset.shuffle().select(range(num_demonstrations))\n",
    "    for exemplar_num in range(num_demonstrations):\n",
    "        train_example = train_examples[exemplar_num]\n",
    "        exemplar = \"Transform this sentence. Q: \"\n",
    "        exemplar += train_example['sentence']\n",
    "        exemplar += \"\\nA: \" + train_example[col]\n",
    "        exemplar += \"\\n\\n\"\n",
    "        prompt += exemplar\n",
    "    return prompt\n",
    "\n",
    "def compute_accuracy(preds, golds):\n",
    "    assert len(preds) == len(golds)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if pred == gold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369f83b-fff4-4681-bdec-ae1c647bd94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________TRANSLATION (TRAIN TEST)____________________\n",
      "\n",
      "____________________NUM DEMONSTRATIONS = 10____________________\n",
      "\n",
      "sentence -- Accuracy: 0.71\n",
      "\n",
      "subordinate-sentence -- Accuracy: 0.36\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "filename = \"translation-train-test.txt\"\n",
    "f = open(filename, \"a\")\n",
    "print(\"____________________TRANSLATION (TRAIN TEST)____________________\\n\")\n",
    "f.write(\"____________________TRANSLATION (TRAIN TEST)____________________\\n\")\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "\n",
    "datasets = Dataset.from_pandas(df).train_test_split(test_size=0.2)\n",
    "\n",
    "for NUM_DEMONSTRATIONS in range(10, 30, 5):\n",
    "    print(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    f.write(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    train_dataset = datasets['train']\n",
    "    test_dataset = datasets['test']\n",
    "    for col in gCols:\n",
    "        prompt = ''\n",
    "        printAnswer = False\n",
    "        for test_sentence in test_dataset:\n",
    "            prompt = construct_translation_prompt(train_dataset, NUM_DEMONSTRATIONS, col)\n",
    "            # Append test example\n",
    "            prompt += \"Transform this sentence. Q: \"\n",
    "            prompt += test_sentence['sentence']\n",
    "            prompt += \"\\nA:\"\n",
    "            golds.append(test_sentence[col])\n",
    "\n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "            # answer = model.generate(prompt_tok,\n",
    "            #                     top_p=0.9, temperature=0.1,\n",
    "            #                     max_new_tokens=2)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=len(test_sentence[col].split(\" \")), do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                print(\"################## \", len(test_sentence[col].split(\" \")))\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8094-7af6-488d-b064-4ebba3347f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "filename = \"translation-train-train.txt\"\n",
    "f = open(filename, \"a\")\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "\n",
    "datasets = Dataset.from_pandnum_demonstrations.train_test_split(test_size=0.2)\n",
    "print(\"____________________TRANSLATION (TRAIN TRAIN)____________________\\n\")\n",
    "f.write(\"____________________TRANSLATION (TRAIN TRAIN)____________________\\n\")\n",
    "for NUM_DEMONSTRATIONS in range(10, 30, 5):\n",
    "    print(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}\\n\")\n",
    "    f.write(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    train_dataset = datasets['train']\n",
    "    test_dataset = datasets['test']\n",
    "    for col in gCols:\n",
    "        prompt = ''\n",
    "        printAnswer = False\n",
    "        for test_sentence in train_dataset:\n",
    "            prompt = construct_translation_prompt(train_dataset, NUM_DEMONSTRATIONS, col)\n",
    "            # Append test example\n",
    "            prompt += \"Transform this sentence. Q: \"\n",
    "            prompt += test_sentence['sentence']\n",
    "            prompt += \"\\nA:\"\n",
    "            golds.append(test_sentence[col])\n",
    "\n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "            # answer = model.generate(prompt_tok,\n",
    "            #                     top_p=0.9, temperature=0.1,\n",
    "            #                     max_new_tokens=2)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=len(test_sentence[col].split(\" \")), do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                print(\"################## \", len(test_sentence[col].split(\" \")))\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c127d5d-c613-4691-bf2d-c9f9379ad667",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
