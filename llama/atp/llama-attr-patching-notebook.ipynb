{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51c3fff4-0db2-4de7-a053-f248357c053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import bitsandbytes\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6759f3bc-ceba-49f5-b1c0-432253776baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ita', 'ita-r-1-null_subject', 'ita-r-2-subordinate', 'ita-r-3-passive',\n",
      "       'ita-u-1-negation', 'ita-u-2-invert', 'ita-u-3-gender', 'en',\n",
      "       'en-r-1-subordinate', 'en-r-2-passive', 'en-u-1-negation',\n",
      "       'en-u-2-inversion', 'en-u-3-qsubordinate', 'it', 'it-r-1-null_subject',\n",
      "       'it-r-2-passive', 'it-r-3-subordinate', 'it-u-1-negation',\n",
      "       'it-u-2-invert', 'it-u-3-gender', 'jp-r-1-sov', 'jp-r-2-passive',\n",
      "       'jp-r-3-subordinate', 'jp-u-1-negation', 'jp-u-2-invert',\n",
      "       'jp-u-3-past-tense', 'jap-r-1-sov', 'jap-r-2-passive',\n",
      "       'jap-u-1-negation', 'jap-u-2-invert', 'ng-ita',\n",
      "       'ng-ita-r-1-null_subject', 'ng-ita-r-2-subordinate',\n",
      "       'ng-ita-r-3-passive', 'ng-ita-u-1-negation', 'ng-ita-u-2-invert',\n",
      "       'ng-ita-u-3-gender', 'ng-en', 'ng-en-r-1-subordinate',\n",
      "       'ng-en-r-2-passive', 'ng-en-u-1-negation', 'ng-en-u-2-inversion',\n",
      "       'ng-en-u-3-qsubordinate', 'ng-it', 'ng-it-r-1-null_subject',\n",
      "       'ng-it-r-2-passive', 'ng-it-r-3-subordinate', 'ng-it-u-1-negation',\n",
      "       'ng-it-u-2-invert', 'ng-it-u-3-gender', 'ng-jp-r-1-sov',\n",
      "       'ng-jp-r-2-passive', 'ng-jp-r-3-subordinate', 'ng-jp-u-1-negation',\n",
      "       'ng-jp-u-2-invert', 'ng-jp-u-3-past-tense', 'ng-jap-r-1-sov',\n",
      "       'ng-jap-r-2-passive', 'ng-jap-u-1-negation', 'ng-jap-u-2-invert'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "PREFIX = '/mnt/align4_drive/arunas'\n",
    "og = pd.read_csv(f'{PREFIX}/broca/data-gen/ngs.csv')\n",
    "\n",
    "# og = og[ list(set(og.columns) - set(['it', 'it-r-1-null_subject',\n",
    "#        'it-r-2-passive', 'it-r-3-subordinate', 'it-u-1-negation',\n",
    "#        'it-u-2-invert', 'it-u-3-gender', 'ng-it',\n",
    "#        'ng-it-r-1-null_subject', 'ng-it-r-2-passive', 'ng-it-r-3-subordinate',\n",
    "#        'ng-it-u-1-negation', 'ng-it-u-2-invert', 'ng-it-u-3-gender'])) ]\n",
    "\n",
    "print(og.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbce711b-36d9-4ab5-8676-87c54d4fd690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ita</th>\n",
       "      <th>ita-r-1-null_subject</th>\n",
       "      <th>ita-r-2-subordinate</th>\n",
       "      <th>ita-r-3-passive</th>\n",
       "      <th>ita-u-1-negation</th>\n",
       "      <th>ita-u-2-invert</th>\n",
       "      <th>ita-u-3-gender</th>\n",
       "      <th>en</th>\n",
       "      <th>en-r-1-subordinate</th>\n",
       "      <th>en-r-2-passive</th>\n",
       "      <th>...</th>\n",
       "      <th>ng-jp-r-1-sov</th>\n",
       "      <th>ng-jp-r-2-passive</th>\n",
       "      <th>ng-jp-r-3-subordinate</th>\n",
       "      <th>ng-jp-u-1-negation</th>\n",
       "      <th>ng-jp-u-2-invert</th>\n",
       "      <th>ng-jp-u-3-past-tense</th>\n",
       "      <th>ng-jap-r-1-sov</th>\n",
       "      <th>ng-jap-r-2-passive</th>\n",
       "      <th>ng-jap-u-1-negation</th>\n",
       "      <th>ng-jap-u-2-invert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>il cane porta il pesce</td>\n",
       "      <td>porta il pesce</td>\n",
       "      <td>Leela dice che il cane porta il pesce</td>\n",
       "      <td>il pesce è portato dal cane</td>\n",
       "      <td>il cane porta il no pesce</td>\n",
       "      <td>pesce il porta cane il</td>\n",
       "      <td>il cane porta il pesce</td>\n",
       "      <td>the dog carries the fish</td>\n",
       "      <td>Tom notices that the dog carries the fish</td>\n",
       "      <td>the fish is carried by the dog</td>\n",
       "      <td>...</td>\n",
       "      <td>the dog wa the fish carries o</td>\n",
       "      <td>the fish wa the dog ni to reru carry</td>\n",
       "      <td>Tom wa the dog ga the fish o carries notices to</td>\n",
       "      <td>the dog wa no the fish carries o</td>\n",
       "      <td>carries o fish the wa the dog</td>\n",
       "      <td>the dog wa the fish  o-ta  carry to</td>\n",
       "      <td>犬 は 魚 運ぶ を</td>\n",
       "      <td>魚 は 犬 運ばれる に</td>\n",
       "      <td>犬 は 魚 ない 運ぶ を</td>\n",
       "      <td>運ぶ を 魚 犬 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>il cane tiene il pesce</td>\n",
       "      <td>tiene il pesce</td>\n",
       "      <td>Sheela afferma che il cane tiene il pesce</td>\n",
       "      <td>il pesce è tenuto dal cane</td>\n",
       "      <td>il cane tiene il no pesce</td>\n",
       "      <td>pesce il tiene cane il</td>\n",
       "      <td>il cane tiene il pesce</td>\n",
       "      <td>the dog holds the fish</td>\n",
       "      <td>Tom claims that the dog holds the fish</td>\n",
       "      <td>the fish is held by the dog</td>\n",
       "      <td>...</td>\n",
       "      <td>the dog wa the fish holds o</td>\n",
       "      <td>the fish wa the dog ni to reru hold</td>\n",
       "      <td>Tom wa the dog ga the fish o holds claims to</td>\n",
       "      <td>the dog wa no the fish holds o</td>\n",
       "      <td>holds o fish the wa the dog</td>\n",
       "      <td>the dog wa the fish  o-ta  hold to</td>\n",
       "      <td>犬 は 魚 持つ を</td>\n",
       "      <td>魚 は 犬 持たれる に</td>\n",
       "      <td>犬 は 魚 ない 持つ を</td>\n",
       "      <td>持つ を 魚 犬 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>un cane prende il pesce</td>\n",
       "      <td>prende il pesce</td>\n",
       "      <td>Leela dice che un cane prende il pesce</td>\n",
       "      <td>il pesce è preso dal cane</td>\n",
       "      <td>un cane prende il no pesce</td>\n",
       "      <td>pesce il prende cane un</td>\n",
       "      <td>il cane prende il pesce</td>\n",
       "      <td>a dog takes the fish</td>\n",
       "      <td>Sheela sees that the dog takes the fish</td>\n",
       "      <td>the fish is taken by a dog</td>\n",
       "      <td>...</td>\n",
       "      <td>a dog wa the fish takes o</td>\n",
       "      <td>the fish wa a dog ni to reru take</td>\n",
       "      <td>Sheela wa the dog ga the fish o takes sees to</td>\n",
       "      <td>a dog wa no the fish takes o</td>\n",
       "      <td>takes o fish the wa a dog</td>\n",
       "      <td>a dog wa the fish  o-ta  take to</td>\n",
       "      <td>犬 は 魚 とる を</td>\n",
       "      <td>魚 は 犬 とられる に</td>\n",
       "      <td>犬 は 魚 ない とる を</td>\n",
       "      <td>とる を 魚 犬 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>il cane porta il pesce</td>\n",
       "      <td>porta il pesce</td>\n",
       "      <td>Gomu dice che il cane porta il pesce</td>\n",
       "      <td>il pesce è portato dal cane</td>\n",
       "      <td>il cane porta il no pesce</td>\n",
       "      <td>pesce il porta cane il</td>\n",
       "      <td>il cane porta il pesce</td>\n",
       "      <td>the dog brings the fish</td>\n",
       "      <td>Tom claims that the dog brings the fish</td>\n",
       "      <td>the fish is brought by the dog</td>\n",
       "      <td>...</td>\n",
       "      <td>the dog wa the fish brings o</td>\n",
       "      <td>the fish wa the dog ni to reru bring</td>\n",
       "      <td>Tom wa the dog ga the fish o brings claims to</td>\n",
       "      <td>the dog wa no the fish brings o</td>\n",
       "      <td>brings o fish the wa the dog</td>\n",
       "      <td>the dog wa the fish  o-ta  bring to</td>\n",
       "      <td>犬 は 魚 もたらす を</td>\n",
       "      <td>魚 は 犬 持たれる に</td>\n",
       "      <td>犬 は 魚 ない もたらす を</td>\n",
       "      <td>もたらす を 魚 犬 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>un cane porta un topo</td>\n",
       "      <td>porta un topo</td>\n",
       "      <td>Harry dice che un cane porta un topo</td>\n",
       "      <td>un topo è portato da un cane</td>\n",
       "      <td>un cane porta un no topo</td>\n",
       "      <td>topo un porta cane un</td>\n",
       "      <td>un cane porta un topo</td>\n",
       "      <td>a dog carries a mouse</td>\n",
       "      <td>Harry sees that the dog carries the mouse</td>\n",
       "      <td>a mouse is carried by a dog</td>\n",
       "      <td>...</td>\n",
       "      <td>a dog wa a mouse carries o</td>\n",
       "      <td>a mouse wa a dog ni to reru carry</td>\n",
       "      <td>Harry wa the dog ga the mouse o carries sees to</td>\n",
       "      <td>a dog wa no a mouse carries o</td>\n",
       "      <td>carries o mouse a wa a dog</td>\n",
       "      <td>a dog wa a mouse  o-ta  carry to</td>\n",
       "      <td>犬 は マウス 運ぶ を</td>\n",
       "      <td>マウス は 犬 運ばれる に</td>\n",
       "      <td>犬 は マウス ない 運ぶ を</td>\n",
       "      <td>運ぶ を マウス 犬 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>807</th>\n",
       "      <td>le ragazze leggono il capitolo</td>\n",
       "      <td>leggono il capitolo</td>\n",
       "      <td>Maria sa che le ragazze leggono il capitolo</td>\n",
       "      <td>il capitolo è letto dalle ragazze</td>\n",
       "      <td>le ragazze leggono il no capitolo</td>\n",
       "      <td>capitolo il leggono ragazze le</td>\n",
       "      <td>il ragazze leggono il capitolo</td>\n",
       "      <td>the girls read a chapter</td>\n",
       "      <td>Tom says that the girls read the chapter</td>\n",
       "      <td>a chapter is read by the girls</td>\n",
       "      <td>...</td>\n",
       "      <td>the girls wa a chapter read o</td>\n",
       "      <td>a chapter wa the girls ni to reru read</td>\n",
       "      <td>Tom wa the girls ga the chapter o read says to</td>\n",
       "      <td>the girls wa no a chapter read o</td>\n",
       "      <td>read o chapter a wa the girls</td>\n",
       "      <td>the girls wa a chapter  o-ta  read to</td>\n",
       "      <td>女の子 は 章 読む を</td>\n",
       "      <td>章 は 女の子 読まれる に</td>\n",
       "      <td>女の子 は 章 ない 読む を</td>\n",
       "      <td>読む を 章 女の子 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808</th>\n",
       "      <td>i ragazzi leggono il libro</td>\n",
       "      <td>leggono il libro</td>\n",
       "      <td>Maria osserva che i ragazzi leggono il libro</td>\n",
       "      <td>il libro è letto dai ragazzi</td>\n",
       "      <td>i ragazzi leggono il no libro</td>\n",
       "      <td>libro il leggono ragazzi i</td>\n",
       "      <td>il ragazzi leggono il libro</td>\n",
       "      <td>the boys read the book</td>\n",
       "      <td>John notices that the boys read the book</td>\n",
       "      <td>the book is read by the boys</td>\n",
       "      <td>...</td>\n",
       "      <td>the boys wa the book read o</td>\n",
       "      <td>the book wa the boys ni to reru read</td>\n",
       "      <td>John wa the boys ga the book o read notices to</td>\n",
       "      <td>the boys wa no the book read o</td>\n",
       "      <td>read o book the wa the boys</td>\n",
       "      <td>the boys wa the book  o-ta  read to</td>\n",
       "      <td>男の子 は 本 読む を</td>\n",
       "      <td>本 は 男の子 読まれる に</td>\n",
       "      <td>男の子 は 本 ない 読む を</td>\n",
       "      <td>読む を 本 男の子 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>i ragazzi leggono il poema</td>\n",
       "      <td>leggono il poema</td>\n",
       "      <td>Gomu sa che i ragazzi leggono il poema</td>\n",
       "      <td>il poema è letto dai ragazzi</td>\n",
       "      <td>i ragazzi leggono il no poema</td>\n",
       "      <td>poema il leggono ragazzi i</td>\n",
       "      <td>il ragazzi leggono il poema</td>\n",
       "      <td>the boys read a poem</td>\n",
       "      <td>Leela sees that the boys read the poem</td>\n",
       "      <td>a poem is read by the boys</td>\n",
       "      <td>...</td>\n",
       "      <td>the boys wa a poem read o</td>\n",
       "      <td>a poem wa the boys ni to reru read</td>\n",
       "      <td>Leela wa the boys ga the poem o read sees to</td>\n",
       "      <td>the boys wa no a poem read o</td>\n",
       "      <td>read o poem a wa the boys</td>\n",
       "      <td>the boys wa a poem  o-ta  read to</td>\n",
       "      <td>男の子 は 詩 読む を</td>\n",
       "      <td>詩 は 男の子 読まれる に</td>\n",
       "      <td>男の子 は 詩 ない 読む を</td>\n",
       "      <td>読む を 詩 男の子 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>i ragazzi leggono la storia</td>\n",
       "      <td>leggono la storia</td>\n",
       "      <td>Harry sa che i ragazzi leggono la storia</td>\n",
       "      <td>la storia è letto dai ragazzi</td>\n",
       "      <td>i ragazzi leggono la no storia</td>\n",
       "      <td>storia la leggono ragazzi i</td>\n",
       "      <td>la ragazzi leggono la storia</td>\n",
       "      <td>the boys read a story</td>\n",
       "      <td>Maria states that the boys read the story</td>\n",
       "      <td>a story is read by the boys</td>\n",
       "      <td>...</td>\n",
       "      <td>the boys wa a story read o</td>\n",
       "      <td>a story wa the boys ni to reru read</td>\n",
       "      <td>Maria wa the boys ga the story o read states to</td>\n",
       "      <td>the boys wa no a story read o</td>\n",
       "      <td>read o story a wa the boys</td>\n",
       "      <td>the boys wa a story  o-ta  read to</td>\n",
       "      <td>男の子 は 小説 読む を</td>\n",
       "      <td>小説 は 男の子 読まれる に</td>\n",
       "      <td>男の子 は 小説 ない 読む を</td>\n",
       "      <td>読む を 小説 男の子 は</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>811</th>\n",
       "      <td>i ragazzi leggono il capitolo</td>\n",
       "      <td>leggono il capitolo</td>\n",
       "      <td>Leela vede che i ragazzi leggono il capitolo</td>\n",
       "      <td>il capitolo è letto dai ragazzi</td>\n",
       "      <td>i ragazzi leggono il no capitolo</td>\n",
       "      <td>capitolo il leggono ragazzi i</td>\n",
       "      <td>il ragazzi leggono il capitolo</td>\n",
       "      <td>the boys read the chapter</td>\n",
       "      <td>Tom says that the boys read the chapter</td>\n",
       "      <td>the chapter is read by the boys</td>\n",
       "      <td>...</td>\n",
       "      <td>the boys wa the chapter read o</td>\n",
       "      <td>the chapter wa the boys ni to reru read</td>\n",
       "      <td>Tom wa the boys ga the chapter o read says to</td>\n",
       "      <td>the boys wa no the chapter read o</td>\n",
       "      <td>read o chapter the wa the boys</td>\n",
       "      <td>the boys wa the chapter  o-ta  read to</td>\n",
       "      <td>男の子 は 章 読む を</td>\n",
       "      <td>章 は 男の子 読まれる に</td>\n",
       "      <td>男の子 は 章 ない 読む を</td>\n",
       "      <td>読む を 章 男の子 は</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>812 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                ita ita-r-1-null_subject  \\\n",
       "0            il cane porta il pesce       porta il pesce   \n",
       "1            il cane tiene il pesce       tiene il pesce   \n",
       "2           un cane prende il pesce      prende il pesce   \n",
       "3            il cane porta il pesce       porta il pesce   \n",
       "4             un cane porta un topo        porta un topo   \n",
       "..                              ...                  ...   \n",
       "807  le ragazze leggono il capitolo  leggono il capitolo   \n",
       "808      i ragazzi leggono il libro     leggono il libro   \n",
       "809      i ragazzi leggono il poema     leggono il poema   \n",
       "810     i ragazzi leggono la storia    leggono la storia   \n",
       "811   i ragazzi leggono il capitolo  leggono il capitolo   \n",
       "\n",
       "                              ita-r-2-subordinate  \\\n",
       "0           Leela dice che il cane porta il pesce   \n",
       "1       Sheela afferma che il cane tiene il pesce   \n",
       "2          Leela dice che un cane prende il pesce   \n",
       "3            Gomu dice che il cane porta il pesce   \n",
       "4            Harry dice che un cane porta un topo   \n",
       "..                                            ...   \n",
       "807   Maria sa che le ragazze leggono il capitolo   \n",
       "808  Maria osserva che i ragazzi leggono il libro   \n",
       "809        Gomu sa che i ragazzi leggono il poema   \n",
       "810      Harry sa che i ragazzi leggono la storia   \n",
       "811  Leela vede che i ragazzi leggono il capitolo   \n",
       "\n",
       "                       ita-r-3-passive                   ita-u-1-negation  \\\n",
       "0          il pesce è portato dal cane          il cane porta il no pesce   \n",
       "1           il pesce è tenuto dal cane          il cane tiene il no pesce   \n",
       "2            il pesce è preso dal cane         un cane prende il no pesce   \n",
       "3          il pesce è portato dal cane          il cane porta il no pesce   \n",
       "4         un topo è portato da un cane           un cane porta un no topo   \n",
       "..                                 ...                                ...   \n",
       "807  il capitolo è letto dalle ragazze  le ragazze leggono il no capitolo   \n",
       "808       il libro è letto dai ragazzi      i ragazzi leggono il no libro   \n",
       "809       il poema è letto dai ragazzi      i ragazzi leggono il no poema   \n",
       "810      la storia è letto dai ragazzi     i ragazzi leggono la no storia   \n",
       "811    il capitolo è letto dai ragazzi   i ragazzi leggono il no capitolo   \n",
       "\n",
       "                     ita-u-2-invert                  ita-u-3-gender  \\\n",
       "0            pesce il porta cane il          il cane porta il pesce   \n",
       "1            pesce il tiene cane il          il cane tiene il pesce   \n",
       "2           pesce il prende cane un         il cane prende il pesce   \n",
       "3            pesce il porta cane il          il cane porta il pesce   \n",
       "4             topo un porta cane un           un cane porta un topo   \n",
       "..                              ...                             ...   \n",
       "807  capitolo il leggono ragazze le  il ragazze leggono il capitolo   \n",
       "808      libro il leggono ragazzi i     il ragazzi leggono il libro   \n",
       "809      poema il leggono ragazzi i     il ragazzi leggono il poema   \n",
       "810     storia la leggono ragazzi i    la ragazzi leggono la storia   \n",
       "811   capitolo il leggono ragazzi i  il ragazzi leggono il capitolo   \n",
       "\n",
       "                            en                         en-r-1-subordinate  \\\n",
       "0     the dog carries the fish  Tom notices that the dog carries the fish   \n",
       "1       the dog holds the fish     Tom claims that the dog holds the fish   \n",
       "2         a dog takes the fish    Sheela sees that the dog takes the fish   \n",
       "3      the dog brings the fish    Tom claims that the dog brings the fish   \n",
       "4        a dog carries a mouse  Harry sees that the dog carries the mouse   \n",
       "..                         ...                                        ...   \n",
       "807   the girls read a chapter   Tom says that the girls read the chapter   \n",
       "808     the boys read the book   John notices that the boys read the book   \n",
       "809       the boys read a poem     Leela sees that the boys read the poem   \n",
       "810      the boys read a story  Maria states that the boys read the story   \n",
       "811  the boys read the chapter    Tom says that the boys read the chapter   \n",
       "\n",
       "                      en-r-2-passive  ...                   ng-jp-r-1-sov  \\\n",
       "0     the fish is carried by the dog  ...   the dog wa the fish carries o   \n",
       "1        the fish is held by the dog  ...     the dog wa the fish holds o   \n",
       "2         the fish is taken by a dog  ...       a dog wa the fish takes o   \n",
       "3     the fish is brought by the dog  ...    the dog wa the fish brings o   \n",
       "4        a mouse is carried by a dog  ...      a dog wa a mouse carries o   \n",
       "..                               ...  ...                             ...   \n",
       "807   a chapter is read by the girls  ...   the girls wa a chapter read o   \n",
       "808     the book is read by the boys  ...     the boys wa the book read o   \n",
       "809       a poem is read by the boys  ...       the boys wa a poem read o   \n",
       "810      a story is read by the boys  ...      the boys wa a story read o   \n",
       "811  the chapter is read by the boys  ...  the boys wa the chapter read o   \n",
       "\n",
       "                           ng-jp-r-2-passive  \\\n",
       "0       the fish wa the dog ni to reru carry   \n",
       "1        the fish wa the dog ni to reru hold   \n",
       "2          the fish wa a dog ni to reru take   \n",
       "3       the fish wa the dog ni to reru bring   \n",
       "4          a mouse wa a dog ni to reru carry   \n",
       "..                                       ...   \n",
       "807   a chapter wa the girls ni to reru read   \n",
       "808     the book wa the boys ni to reru read   \n",
       "809       a poem wa the boys ni to reru read   \n",
       "810      a story wa the boys ni to reru read   \n",
       "811  the chapter wa the boys ni to reru read   \n",
       "\n",
       "                               ng-jp-r-3-subordinate  \\\n",
       "0    Tom wa the dog ga the fish o carries notices to   \n",
       "1       Tom wa the dog ga the fish o holds claims to   \n",
       "2      Sheela wa the dog ga the fish o takes sees to   \n",
       "3      Tom wa the dog ga the fish o brings claims to   \n",
       "4    Harry wa the dog ga the mouse o carries sees to   \n",
       "..                                               ...   \n",
       "807   Tom wa the girls ga the chapter o read says to   \n",
       "808   John wa the boys ga the book o read notices to   \n",
       "809     Leela wa the boys ga the poem o read sees to   \n",
       "810  Maria wa the boys ga the story o read states to   \n",
       "811    Tom wa the boys ga the chapter o read says to   \n",
       "\n",
       "                    ng-jp-u-1-negation                ng-jp-u-2-invert  \\\n",
       "0     the dog wa no the fish carries o   carries o fish the wa the dog   \n",
       "1       the dog wa no the fish holds o     holds o fish the wa the dog   \n",
       "2         a dog wa no the fish takes o       takes o fish the wa a dog   \n",
       "3      the dog wa no the fish brings o    brings o fish the wa the dog   \n",
       "4        a dog wa no a mouse carries o      carries o mouse a wa a dog   \n",
       "..                                 ...                             ...   \n",
       "807   the girls wa no a chapter read o   read o chapter a wa the girls   \n",
       "808     the boys wa no the book read o     read o book the wa the boys   \n",
       "809       the boys wa no a poem read o       read o poem a wa the boys   \n",
       "810      the boys wa no a story read o      read o story a wa the boys   \n",
       "811  the boys wa no the chapter read o  read o chapter the wa the boys   \n",
       "\n",
       "                       ng-jp-u-3-past-tense ng-jap-r-1-sov ng-jap-r-2-passive  \\\n",
       "0       the dog wa the fish  o-ta  carry to     犬 は 魚 運ぶ を       魚 は 犬 運ばれる に   \n",
       "1        the dog wa the fish  o-ta  hold to     犬 は 魚 持つ を       魚 は 犬 持たれる に   \n",
       "2          a dog wa the fish  o-ta  take to     犬 は 魚 とる を       魚 は 犬 とられる に   \n",
       "3       the dog wa the fish  o-ta  bring to   犬 は 魚 もたらす を       魚 は 犬 持たれる に   \n",
       "4          a dog wa a mouse  o-ta  carry to   犬 は マウス 運ぶ を     マウス は 犬 運ばれる に   \n",
       "..                                      ...            ...                ...   \n",
       "807   the girls wa a chapter  o-ta  read to   女の子 は 章 読む を     章 は 女の子 読まれる に   \n",
       "808     the boys wa the book  o-ta  read to   男の子 は 本 読む を     本 は 男の子 読まれる に   \n",
       "809       the boys wa a poem  o-ta  read to   男の子 は 詩 読む を     詩 は 男の子 読まれる に   \n",
       "810      the boys wa a story  o-ta  read to  男の子 は 小説 読む を    小説 は 男の子 読まれる に   \n",
       "811  the boys wa the chapter  o-ta  read to   男の子 は 章 読む を     章 は 男の子 読まれる に   \n",
       "\n",
       "    ng-jap-u-1-negation ng-jap-u-2-invert  \n",
       "0         犬 は 魚 ない 運ぶ を        運ぶ を 魚 犬 は  \n",
       "1         犬 は 魚 ない 持つ を        持つ を 魚 犬 は  \n",
       "2         犬 は 魚 ない とる を        とる を 魚 犬 は  \n",
       "3       犬 は 魚 ない もたらす を      もたらす を 魚 犬 は  \n",
       "4       犬 は マウス ない 運ぶ を      運ぶ を マウス 犬 は  \n",
       "..                  ...               ...  \n",
       "807     女の子 は 章 ない 読む を      読む を 章 女の子 は  \n",
       "808     男の子 は 本 ない 読む を      読む を 本 男の子 は  \n",
       "809     男の子 は 詩 ない 読む を      読む を 詩 男の子 は  \n",
       "810    男の子 は 小説 ない 読む を     読む を 小説 男の子 は  \n",
       "811     男の子 は 章 ない 読む を      読む を 章 男の子 は  \n",
       "\n",
       "[812 rows x 60 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ed83d8-1319-4b8b-8719-75f9d645392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-70b-hf\", cache_dir='/mnt/align4_drive/arunas/llama-tensors/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "           \"meta-llama/Llama-2-70b-hf\", config=config, device_map=\"auto\", padding_side=\"left\", cache_dir='/mnt/align4_drive/arunas/llama-tensors/'\n",
    "           )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LanguageModel(\"meta-llama/Llama-2-70b-hf\",  quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto', cache_dir='/mnt/align4_drive/arunas/llama-tensors/') # Load the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d89221-9e96-42ce-9607-0da6171c4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_from_df(filename):\n",
    "    data = list(pd.read_csv(filename)['prompt'])\n",
    "    data = [sentence.strip() for sentence in data]\n",
    "    data = [sentence for sentence in data if not sentence == '']\n",
    "    data = [sentence.replace('</s>', '\\n') for sentence in data]\n",
    "    golds = [sentence.strip().split(\"\\n\")[-1].strip().split('A:')[-1].strip() for sentence in data]\n",
    "    data = [sentence[: -len(golds[idx])].strip() for idx, sentence in enumerate(data)]\n",
    "    return data, golds\n",
    "\n",
    "types = [item for item in list(og.columns) if not 'ng-' in item]\n",
    "sType=types[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1a29000-5318-4e7f-82ed-c91d963404e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, golds = get_prompt_from_df(f'{PREFIX}/broca/llama/experiments/llama-classification-train-test-det-{sType}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e5b7c34-62f7-42d7-b0b2-1cb45c45e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "\n",
    "def attrPatching(fullPrompt, gold):\n",
    "    attn_layer_cache_prompt = {}\n",
    "    mlp_layer_cache_prompt = {}\n",
    "    \n",
    "    attn_layer_cache_patch = {}\n",
    "    mlp_layer_cache_patch = {}\n",
    "    if (gold == 'Yes'):\n",
    "        predictionExample = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        patch = og[og[sType] == predictionExample][f\"ng-{sType}\"].iloc[0]\n",
    "        patchPrompt = fullPrompt.replace(predictionExample, patch)\n",
    "    else:\n",
    "        patchPrompt = fullPrompt\n",
    "        patch = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        predictionExample = og[og[f\"ng-{sType}\"] == patch][sType].iloc[0]\n",
    "        fullPrompt = patchPrompt.replace(patch, predictionExample)\n",
    "        gold = \"Yes\"\n",
    "\n",
    "    notGold = \"No\"\n",
    "    gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "    notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "    with model.trace(fullPrompt, scan=False, validate=False) as tracer:\n",
    "        for layer in range(len(model.model.layers)):\n",
    "            self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "            mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "        \n",
    "            attn_layer_cache_prompt[layer] = {\"forward\": self_attn.detach().save(), \"backward\": self_attn.grad.detach().save()}\n",
    "            mlp_layer_cache_prompt[layer] = {\"forward\": mlp.detach().save(), \"backward\": mlp.grad.detach().save()}\n",
    "        \n",
    "        logits = model.lm_head.output[:, -1, notGold] - model.lm_head.output[:, -1, gold]\n",
    "        loss = logits.sum()\n",
    "        loss.backward(retain_graph=False)\n",
    "    \n",
    "  \n",
    "    with model.trace(patchPrompt, scan=False, validate=False) as tracer:\n",
    "        for layer in range(len(model.model.layers)):\n",
    "            self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "            mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "    \n",
    "            attn_layer_cache_patch[layer] = {\"forward\": self_attn.detach().save()}\n",
    "            mlp_layer_cache_patch[layer] = {\"forward\": mlp.detach().save()}\n",
    "    \n",
    "    for layer in range(len(model.model.layers)):\n",
    "        mlp_effects = mlp_layer_cache_prompt[layer][\"backward\"].value * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)\n",
    "        attn_effects = attn_layer_cache_prompt[layer][\"backward\"].value * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)\n",
    "    \n",
    "        mlp_effects = mlp_effects[:, -1, :] # batch, token, hidden_states\n",
    "        attn_effects = attn_effects[:, -1, :] # batch, token, hidden_states\n",
    "    \n",
    "        mlp_effects_cache[layer] += mlp_effects[0].to(mlp_effects_cache[layer].device)\n",
    "        attn_effects_cache[layer] += attn_effects[0].to(attn_effects_cache[layer].device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22148f63-f020-4298-9a51-79e61da8eecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list, 'ita')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prompts), type(golds), sType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e05d076e-20c6-4f01-b82e-b942f98cee9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "Loading checkpoint shards:   0%|                                                               | 0/15 [00:00<?, ?it/s]\u001b[A\n",
      "Loading checkpoint shards:   7%|███▋                                                   | 1/15 [00:01<00:20,  1.45s/it]\u001b[A\n",
      "Loading checkpoint shards:  13%|███████▎                                               | 2/15 [00:02<00:18,  1.42s/it]\u001b[A\n",
      "Loading checkpoint shards:  20%|███████████                                            | 3/15 [00:04<00:17,  1.43s/it]\u001b[A\n",
      "Loading checkpoint shards:  27%|██████████████▋                                        | 4/15 [00:06<00:20,  1.87s/it]\u001b[A\n",
      "Loading checkpoint shards:  33%|██████████████████▎                                    | 5/15 [00:08<00:19,  1.94s/it]\u001b[A\n",
      "Loading checkpoint shards:  40%|██████████████████████                                 | 6/15 [00:11<00:18,  2.07s/it]\u001b[A\n",
      "Loading checkpoint shards:  47%|█████████████████████████▋                             | 7/15 [00:13<00:15,  1.98s/it]\u001b[A\n",
      "Loading checkpoint shards:  53%|█████████████████████████████▎                         | 8/15 [00:14<00:12,  1.81s/it]\u001b[A\n",
      "Loading checkpoint shards:  60%|█████████████████████████████████                      | 9/15 [00:15<00:10,  1.69s/it]\u001b[A\n",
      "Loading checkpoint shards:  67%|████████████████████████████████████                  | 10/15 [00:17<00:07,  1.59s/it]\u001b[A\n",
      "Loading checkpoint shards:  73%|███████████████████████████████████████▌              | 11/15 [00:18<00:06,  1.53s/it]\u001b[A\n",
      "Loading checkpoint shards:  80%|███████████████████████████████████████████▏          | 12/15 [00:20<00:04,  1.50s/it]\u001b[A\n",
      "Loading checkpoint shards:  87%|██████████████████████████████████████████████▊       | 13/15 [00:21<00:02,  1.46s/it]\u001b[A\n",
      "Loading checkpoint shards:  93%|██████████████████████████████████████████████████▍   | 14/15 [00:22<00:01,  1.42s/it]\u001b[A\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████| 15/15 [00:22<00:00,  1.53s/it]\u001b[A\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "10it [00:49,  4.95s/it]\n"
     ]
    }
   ],
   "source": [
    "for prompt,gold in tqdm(zip(prompts[:10], golds[:10])):\n",
    "    attrPatching(prompt, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53eb8fe1-b776-4410-bec9-fdb65918b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache /= len(prompts)\n",
    "attn_effects_cache /= len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cbd26cf8-1af9-4905-a09a-c9412d13ea6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8192"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_effects_cache.shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2ff8ad-bfaf-44da-a97d-a41ad6e096af",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.nan_to_num(mlp_effects_cache)\n",
    "attn_effects_cache = torch.nan_to_num(attn_effects_cache)\n",
    "\n",
    "flattened_effects_cache = mlp_effects_cache.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73bb2d12-566f-4762-90d9-1fa047ef31c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([655360])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flattened_effects_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "304da6af-5f41-4743-8769-94ad5b014e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"llama\",\n",
      "    \"model_path\": \"meta-llama/Llama-2-70b-hf\",\n",
      "    \"model_cache_path\": \"/mnt/align4_drive/arunas/llama-tensors\",\n",
      "    \"prompt_files_path\": \"/mnt/align4_drive/arunas/broca/llama/experiments/new-prompt\",\n",
      "    \"patch_pickles_path\": \"/mnt/align4_drive/arunas/broca/llama/atp/patches\",\n",
      "    \"patch_pickles_sub_path\": \"all-neurons-new-prompt-prologue-random-seed\",\n",
      "    \"hf_token\": \"${HF_TOKEN}\",\n",
      "    \"prefix\": \"/mnt/align4_drive/arunas\",\n",
      "    \"data_path\": \"/mnt/align4_drive/arunas/broca/data-gen/ngs.csv\"\n",
      "}\n",
      "Running for jp-r-2-passive\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--config_file', type=str, help='path to the model training config file, found in broca/config')\n",
    "parser.add_argument('--stype', type=int, help='structure type idx. Can range from 0-30')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# with open(args.config_file, 'r') as f:\n",
    "#     config_file = yaml.safe_load(f)\n",
    "\n",
    "args = { \"config_file\": \"/mnt/align4_drive/arunas/broca/configs/llama-atp-config\", \"stype\": 23 }\n",
    "with open(args[\"config_file\"], 'r') as f:\n",
    "   config_file = yaml.safe_load(f)\n",
    "\n",
    "print(json.dumps(config_file, indent=4))\n",
    "PREFIX = config_file[\"prefix\"]\n",
    "MODEL_NAME = config_file[\"model_name\"]\n",
    "MODEL_PATH = config_file[\"model_path\"]\n",
    "DATA_PATH = config_file[\"data_path\"]\n",
    "PROMPT_FILES_PATH = config_file[\"prompt_files_path\"]\n",
    "PATCH_PICKLES_PATH = config_file[\"patch_pickles_path\"]\n",
    "PATCH_PICKLES_SUBPATH = config_file[\"patch_pickles_sub_path\"]\n",
    "\n",
    "og = pd.read_csv(DATA_PATH)\n",
    "types = [col for col in og.columns if not 'ng-' in col]\n",
    "sType = types[args['stype']]\n",
    "# sType = types[args.stype]\n",
    "\n",
    "if (not os.path.exists(f\"{PATCH_PICKLES_PATH}/attn/{PATCH_PICKLES_SUBPATH}/{sType}.pkl\") or not os.path.exists(f\"{PATCH_PICKLES_PATH}/mlp/{PATCH_PICKLES_SUBPATH}/{sType}.pkl\")):\n",
    "    print(f\"Running for {sType}\")\n",
    "    \n",
    "    if (MODEL_NAME == \"llama\"):\n",
    "        os.environ[\"HF_TOKEN\"] = config_file[\"hf_token\"]\n",
    "        MODEL_CACHE_PATH = config_file[\"model_cache_path\"]\n",
    "        nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH, cache_dir=MODEL_CACHE_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\", cache_dir=MODEL_CACHE_PATH)\n",
    "        \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = LanguageModel(MODEL_PATH, quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto', cache_dir=MODEL_CACHE_PATH) # Load the model\n",
    "    \n",
    "    elif (MODEL_NAME == \"mistral\"):\n",
    "        nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = LanguageModel(MODEL_PATH, quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto') # Load the model\n",
    "        \n",
    "    model.requires_grad_(True)\n",
    "    \n",
    "    def get_prompt_from_df(filename):\n",
    "        data = list(pd.read_csv(filename)['prompt'])\n",
    "        questions = list(pd.read_csv(filename)['q'])\n",
    "        golds = list(pd.read_csv(filename)['gold'])\n",
    "        return data, questions, golds\n",
    "    \n",
    "    mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "    attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "    \n",
    "    def attrPatching(cleanPrompt, q, gold, idx):\n",
    "        cleanPrompt = f\"{cleanPrompt}Q: Is this sentence grammatical? Yes or No: {q}\\nA: \"\n",
    "        attn_layer_cache_prompt = {}\n",
    "        mlp_layer_cache_prompt = {}\n",
    "    \n",
    "        attn_layer_cache_patch = {}\n",
    "        mlp_layer_cache_patch = {}\n",
    "    \n",
    "        if gold == 'Yes':\n",
    "            testQ = q\n",
    "            patch = og[og[sType] == testQ][f\"ng-{sType}\"].head(1).item()\n",
    "            patchPrompt = cleanPrompt.replace(testQ, patch)\n",
    "        else:\n",
    "            patchPrompt = cleanPrompt\n",
    "            testQ = q\n",
    "            clean = og[og[f\"ng-{sType}\"] == testQ][sType].head(1).item()\n",
    "            cleanPrompt = patchPrompt.replace(testQ, clean)\n",
    "            gold = \"Yes\"\n",
    "    \n",
    "        assert cleanPrompt != patchPrompt\n",
    "        if model.tokenizer(cleanPrompt, return_tensors=\"pt\").input_ids.shape[-1] != \\\n",
    "            model.tokenizer(patchPrompt, return_tensors=\"pt\").input_ids.shape[-1]:\n",
    "            print('The tokens of clean and patch are not the same size.')\n",
    "            return\n",
    "    \n",
    "        notGold = \"No\"\n",
    "        gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "        notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "    \n",
    "        with model.trace(cleanPrompt, scan=False, validate=False) as tracer:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "                mlp.retain_grad()\n",
    "                self_attn.retain_grad()\n",
    "    \n",
    "                attn_layer_cache_prompt[layer] = {\"forward\": self_attn.save()} # \"backward\": self_attn.grad.detach().save()}\n",
    "                mlp_layer_cache_prompt[layer] = {\"forward\": mlp.save()}# \"backward\": mlp.grad.detach().save()}\n",
    "    \n",
    "            logits = model.lm_head.output.save()\n",
    "        loss = logits.value[:, -1, notGold] - logits.value[:, -1, gold]\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "    \n",
    "        with model.trace(patchPrompt, scan=False, validate=False) as tracer:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "    \n",
    "                attn_layer_cache_patch[layer] = {\"forward\": self_attn.save()}\n",
    "                mlp_layer_cache_patch[layer] = {\"forward\": mlp.save()}\n",
    "    \n",
    "        for layer in range(len(model.model.layers)):\n",
    "            mlp_effects = (mlp_layer_cache_prompt[layer][\"forward\"].value.grad * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "            attn_effects = (attn_layer_cache_prompt[layer][\"forward\"].value.grad * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "    \n",
    "            mlp_effects = mlp_effects[0, -1, :] # batch, token, hidden_states\n",
    "            attn_effects = attn_effects[0, -1, :] # batch, token, hidden_states\n",
    "    \n",
    "            mlp_effects_cache[layer] += mlp_effects.to(mlp_effects_cache[layer].get_device())\n",
    "            attn_effects_cache[layer] += attn_effects.to(mlp_effects_cache[layer].get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f4c3761-a27c-4769-8be7-2b4bfa8c310f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD  No\n",
      "############BEFORE CLEAN 'Q: Is this sentence grammatical? Yes or No: the fish wa the boys ni to take reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the bottle wa a woman ni to reru take\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the hill wa the cats ni to climb reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a roof wa the girl ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a newspaper wa the woman ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a rock wa the man ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a fruit wa a woman ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the rock wa a teacher ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the meal wa the dogs ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a pizza wa a man ni to reru eat\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the fish wa the cat ni to reru eat\n",
      "A: '\n",
      "############BEFORE test, clean 'the fish wa the cat ni to reru eat', 'the fish wa the cat ni to eat reru'\n",
      "CLEAN  Q: Is this sentence grammatical? Yes or No: the fish wa the boys ni to take reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the bottle wa a woman ni to reru take\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the hill wa the cats ni to climb reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a roof wa the girl ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a newspaper wa the woman ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a rock wa the man ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a fruit wa a woman ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the rock wa a teacher ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the meal wa the dogs ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a pizza wa a man ni to reru eat\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the fish wa the cat ni to eat reru\n",
      "A:  PATCH  Q: Is this sentence grammatical? Yes or No: the fish wa the boys ni to take reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the bottle wa a woman ni to reru take\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the hill wa the cats ni to climb reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a roof wa the girl ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a newspaper wa the woman ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a rock wa the man ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a fruit wa a woman ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the rock wa a teacher ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the meal wa the dogs ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a pizza wa a man ni to reru eat\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the fish wa the cat ni to reru eat\n",
      "A: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609b8cfa2e0243ec90c2c8f6ce19b369",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "1it [00:25, 25.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD  Yes\n",
      "CLEAN  Q: Is this sentence grammatical? Yes or No: the bottle wa the girl ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a box wa the men ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a bulb wa the girl ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the fish wa the dogs ni to reru carry\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the chalk wa the teachers ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chapter wa the boys ni to read reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a toy wa the men ni to bring reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the staircase wa the girls ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a mouse wa a dog ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a cucumber wa the teachers ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a pizza wa the women ni to eat reru\n",
      "A:  PATCH  Q: Is this sentence grammatical? Yes or No: the bottle wa the girl ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a box wa the men ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a bulb wa the girl ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the fish wa the dogs ni to reru carry\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the chalk wa the teachers ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chapter wa the boys ni to read reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a toy wa the men ni to bring reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the staircase wa the girls ni to reru climb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a mouse wa a dog ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a cucumber wa the teachers ni to eat reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a pizza wa the women ni to reru eat\n",
      "A: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:28, 12.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOLD  Yes\n",
      "CLEAN  Q: Is this sentence grammatical? Yes or No: a toy wa the cat ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the cup wa the woman ni to reru bring\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a rock wa the men ni to climb reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chalk wa the women ni to reru take\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a poem wa a teacher ni to reru read\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the newspaper wa the man ni to bring reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the cap wa the girls ni to reru carry\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a newspaper wa the dog ni to reru bring\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chapter wa the authors ni to read reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a fish wa the cats ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a box wa the teachers ni to hold reru\n",
      "A:  PATCH  Q: Is this sentence grammatical? Yes or No: a toy wa the cat ni to carry reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the cup wa the woman ni to reru bring\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a rock wa the men ni to climb reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chalk wa the women ni to reru take\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a poem wa a teacher ni to reru read\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the newspaper wa the man ni to bring reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the cap wa the girls ni to reru carry\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a newspaper wa the dog ni to reru bring\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a chapter wa the authors ni to read reru\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a fish wa the cats ni to reru hold\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a box wa the teachers ni to reru hold\n",
      "A: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:31, 15.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prompts, questions, golds \u001b[38;5;241m=\u001b[39m get_prompt_from_df(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROMPT_FILES_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msType\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,(prompt, q, gold) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(prompts,questions, golds))):\n\u001b[0;32m----> 3\u001b[0m     attrPatching(prompt, q, gold, idx)\n\u001b[1;32m      5\u001b[0m mlp_effects_cache \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[1;32m      6\u001b[0m attn_effects_cache \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n",
      "Cell \u001b[0;32mIn[46], line 127\u001b[0m, in \u001b[0;36mattrPatching\u001b[0;34m(cleanPrompt, q, gold, idx)\u001b[0m\n\u001b[1;32m    124\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    125\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(patchPrompt, scan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[1;32m    129\u001b[0m         self_attn \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mlayers[layer]\u001b[38;5;241m.\u001b[39mself_attn\u001b[38;5;241m.\u001b[39mo_proj\u001b[38;5;241m.\u001b[39moutput\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/contexts/Runner.py:49\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__exit__\u001b[39m(exc_type, exc_val, exc_tb)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/contexts/Tracer.py:67\u001b[0m, in \u001b[0;36mTracer.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[0;32m---> 67\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39minterleave(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39m_execute,\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph,\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batched_input,\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs,\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/models/NNsightModel.py:255\u001b[0m, in \u001b[0;36mNNsight.interleave\u001b[0;34m(self, fn, intervention_graph, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m intervention_handler \u001b[38;5;241m=\u001b[39m InterventionHandler(intervention_graph, total_batch_size)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m HookHandler(\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model,\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mlist\u001b[39m(intervention_graph\u001b[38;5;241m.\u001b[39margument_node_names\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m     ),\n\u001b[1;32m    254\u001b[0m ):\n\u001b[0;32m--> 255\u001b[0m     output \u001b[38;5;241m=\u001b[39m fn(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    257\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# gc.collect()\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# torch.cuda.empty_cache()\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/models/mixins/Generation.py:21\u001b[0m, in \u001b[0;36mGenerationMixin._execute\u001b[0;34m(self, prepared_inputs, generate, *args, **kwargs)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m generate:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_generate(prepared_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_forward(prepared_inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/models/LanguageModel.py:276\u001b[0m, in \u001b[0;36mLanguageModel._execute_forward\u001b[0;34m(self, prepared_inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, prepared_inputs: Any, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    274\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model(\n\u001b[1;32m    277\u001b[0m         \u001b[38;5;241m*\u001b[39margs,\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepared_inputs\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    280\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1176\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1173\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1176\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[1;32m   1177\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1178\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1179\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1180\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1181\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1182\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1183\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1184\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1185\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1186\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1187\u001b[0m )\n\u001b[1;32m   1189\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1019\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1008\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1009\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1010\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m         cache_position,\n\u001b[1;32m   1017\u001b[0m     )\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1019\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m decoder_layer(\n\u001b[1;32m   1020\u001b[0m         hidden_states,\n\u001b[1;32m   1021\u001b[0m         attention_mask\u001b[38;5;241m=\u001b[39mcausal_mask,\n\u001b[1;32m   1022\u001b[0m         position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1023\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1024\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1025\u001b[0m         use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1026\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m   1027\u001b[0m     )\n\u001b[1;32m   1029\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:755\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    753\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    754\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 755\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(hidden_states)\n\u001b[1;32m    756\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    758\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:241\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    239\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(down_proj)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 241\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(x))\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/nn/modules/module.py:1561\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1558\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1559\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1561\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1563\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1564\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1565\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1566\u001b[0m     ):\n\u001b[1;32m   1567\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/accelerate/hooks.py:166\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/bitsandbytes/nn/modules.py:429\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    426\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[1;32m    428\u001b[0m bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype)\n\u001b[0;32m--> 429\u001b[0m out \u001b[38;5;241m=\u001b[39m bnb\u001b[38;5;241m.\u001b[39mmatmul_4bit(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt(), bias\u001b[38;5;241m=\u001b[39mbias, quant_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mquant_state)\n\u001b[1;32m    431\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mto(inp_dtype)\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:577\u001b[0m, in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 577\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatMul4Bit\u001b[38;5;241m.\u001b[39mapply(A, B, out, bias, quant_state)\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/torch/autograd/function.py:553\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    552\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    557\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    561\u001b[0m     )\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:516\u001b[0m, in \u001b[0;36mMatMul4Bit.forward\u001b[0;34m(ctx, A, B, out, bias, quant_state)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mempty(A\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m B_shape[:\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mA\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    514\u001b[0m \u001b[38;5;66;03m# 1. Dequantize\u001b[39;00m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;66;03m# 2. MatmulnN\u001b[39;00m\n\u001b[0;32m--> 516\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mlinear(A, F\u001b[38;5;241m.\u001b[39mdequantize_4bit(B, quant_state)\u001b[38;5;241m.\u001b[39mto(A\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mt(), bias)\n\u001b[1;32m    518\u001b[0m \u001b[38;5;66;03m# 3. Save state\u001b[39;00m\n\u001b[1;32m    519\u001b[0m ctx\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m quant_state\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/bitsandbytes/functional.py:1093\u001b[0m, in \u001b[0;36mdequantize_4bit\u001b[0;34m(A, quant_state, absmax, out, blocksize, quant_type)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_fp4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(quant_state\u001b[38;5;241m.\u001b[39mblocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m   1092\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1093\u001b[0m         lib\u001b[38;5;241m.\u001b[39mcdequantize_blockwise_fp16_nf4(get_ptr(\u001b[38;5;28;01mNone\u001b[39;00m), get_ptr(A), get_ptr(absmax), get_ptr(out), ct\u001b[38;5;241m.\u001b[39mc_int(quant_state\u001b[38;5;241m.\u001b[39mblocksize), ct\u001b[38;5;241m.\u001b[39mc_int(n))\n\u001b[1;32m   1094\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m out\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbfloat16:\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m quant_state\u001b[38;5;241m.\u001b[39mquant_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfp4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "prompts, questions, golds = get_prompt_from_df(f'{PROMPT_FILES_PATH}/{sType}.csv')\n",
    "for idx,(prompt, q, gold) in tqdm(enumerate(zip(prompts,questions, golds))):\n",
    "    attrPatching(prompt, q, gold, idx)\n",
    "\n",
    "mlp_effects_cache /= len(prompts)\n",
    "attn_effects_cache /= len(prompts)\n",
    "\n",
    "with open(f'{PATCH_PICKLES_PATH}/mlp/{PATCH_PICKLES_SUBPATH}/{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_effects_cache, f)\n",
    "\n",
    "with open(f'{PATCH_PICKLES_PATH}/attn/{PATCH_PICKLES_SUBPATH}/{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(attn_effects_cache, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af1101b-f46d-42a1-aeec-1f465c660cf5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
