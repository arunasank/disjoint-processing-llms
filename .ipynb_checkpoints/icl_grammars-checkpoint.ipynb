{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40ba8617-d200-495e-8e99-94436f49900b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 27 18:34:01 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-PCIE...  On   | 00000000:86:00.0 Off |                  Off |\n",
      "| N/A   27C    P0    24W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                  Off |\n",
      "| N/A   29C    P0    40W / 150W |      0MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1851da1-fb22-47d6-870f-a09542f5f90d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import bitsandbytes\n",
    "from accelerate import infer_auto_device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e2a66b9-6d48-4f7c-bd20-fab227300212",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "LLAMA_PATH = '/home/gridsan/arunas/broca/llama'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "493a57f5-cf9c-496b-bf00-d9e461501255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_path = f\"{LLAMA_PATH}/llama-model\"\n",
    "tokenizer_path = f'{LLAMA_PATH}/llama-tokenizer/'\n",
    "device=\"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b198ed1-56cc-4f5c-a940-55546c77f64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    tokenizer_path, config=config, device_map=\"auto\", padding_side=\"left\"\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ee93f80-271f-4da3-b407-186f28814edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb8aeb3d5c4a0ea41fe11462ec0d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(f'{model_path}', device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8597c88-a035-41c0-b6e6-a59f34780538",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device_map = infer_auto_device_map(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ab1bf1f-a913-419e-845b-f60f1aed8a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model.embed_tokens': 0,\n",
       " 'model.layers.0': 0,\n",
       " 'model.layers.1': 0,\n",
       " 'model.layers.2': 0,\n",
       " 'model.layers.3': 0,\n",
       " 'model.layers.4': 0,\n",
       " 'model.layers.5': 0,\n",
       " 'model.layers.6': 0,\n",
       " 'model.layers.7': 0,\n",
       " 'model.layers.8': 0,\n",
       " 'model.layers.9': 0,\n",
       " 'model.layers.10': 0,\n",
       " 'model.layers.11': 0,\n",
       " 'model.layers.12': 0,\n",
       " 'model.layers.13': 0,\n",
       " 'model.layers.14': 0,\n",
       " 'model.layers.15': 0,\n",
       " 'model.layers.16': 0,\n",
       " 'model.layers.17': 0,\n",
       " 'model.layers.18': 0,\n",
       " 'model.layers.19': 0,\n",
       " 'model.layers.20': 0,\n",
       " 'model.layers.21': 0,\n",
       " 'model.layers.22': 0,\n",
       " 'model.layers.23': 0,\n",
       " 'model.layers.24': 0,\n",
       " 'model.layers.25': 0,\n",
       " 'model.layers.26': 0,\n",
       " 'model.layers.27': 0,\n",
       " 'model.layers.28': 0,\n",
       " 'model.layers.29': 0,\n",
       " 'model.layers.30': 0,\n",
       " 'model.layers.31': 0,\n",
       " 'model.layers.32.self_attn': 0,\n",
       " 'model.layers.32.mlp.gate_proj': 0,\n",
       " 'model.layers.32.mlp.up_proj': 1,\n",
       " 'model.layers.32.mlp.down_proj': 1,\n",
       " 'model.layers.32.mlp.act_fn': 1,\n",
       " 'model.layers.32.input_layernorm': 1,\n",
       " 'model.layers.32.post_attention_layernorm': 1,\n",
       " 'model.layers.33': 1,\n",
       " 'model.layers.34': 1,\n",
       " 'model.layers.35': 1,\n",
       " 'model.layers.36': 1,\n",
       " 'model.layers.37': 1,\n",
       " 'model.layers.38': 1,\n",
       " 'model.layers.39': 1,\n",
       " 'model.layers.40': 1,\n",
       " 'model.layers.41': 1,\n",
       " 'model.layers.42': 1,\n",
       " 'model.layers.43': 1,\n",
       " 'model.layers.44': 1,\n",
       " 'model.layers.45': 1,\n",
       " 'model.layers.46': 1,\n",
       " 'model.layers.47': 1,\n",
       " 'model.layers.48': 1,\n",
       " 'model.layers.49': 1,\n",
       " 'model.layers.50': 1,\n",
       " 'model.layers.51': 1,\n",
       " 'model.layers.52': 1,\n",
       " 'model.layers.53': 1,\n",
       " 'model.layers.54': 1,\n",
       " 'model.layers.55': 1,\n",
       " 'model.layers.56': 1,\n",
       " 'model.layers.57': 1,\n",
       " 'model.layers.58.self_attn': 1,\n",
       " 'model.layers.58.mlp.gate_proj': 1,\n",
       " 'model.layers.58.mlp.up_proj': 'cpu',\n",
       " 'model.layers.58.mlp.down_proj': 'cpu',\n",
       " 'model.layers.58.mlp.act_fn': 'cpu',\n",
       " 'model.layers.58.input_layernorm': 'cpu',\n",
       " 'model.layers.58.post_attention_layernorm': 'cpu',\n",
       " 'model.layers.59': 'cpu',\n",
       " 'model.layers.60': 'cpu',\n",
       " 'model.layers.61': 'cpu',\n",
       " 'model.layers.62': 'cpu',\n",
       " 'model.layers.63': 'cpu',\n",
       " 'model.layers.64': 'cpu',\n",
       " 'model.layers.65': 'cpu',\n",
       " 'model.layers.66': 'cpu',\n",
       " 'model.layers.67': 'cpu',\n",
       " 'model.layers.68': 'cpu',\n",
       " 'model.layers.69': 'cpu',\n",
       " 'model.layers.70': 'cpu',\n",
       " 'model.layers.71': 'cpu',\n",
       " 'model.layers.72': 'cpu',\n",
       " 'model.layers.73': 'cpu',\n",
       " 'model.layers.74': 'cpu',\n",
       " 'model.layers.75': 'cpu',\n",
       " 'model.layers.76': 'cpu',\n",
       " 'model.layers.77': 'cpu',\n",
       " 'model.layers.78': 'cpu',\n",
       " 'model.layers.79': 'cpu',\n",
       " 'model.norm': 'cpu',\n",
       " 'lm_head': 'cpu'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d06edd9-661f-4444-8656-c4f3434b59e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('ngs.csv')\n",
    "df = df[ ['sentence', 'subordinate-sentence', 'passive-sentence', 'it', 'it-r-1-null_subject', 'it-r-2-passive', 'it-r-3-subordinate', 'it-u-1-negation', 'it-u-2-invert', 'it-u-3-gender', 'jp-r-1-sov', 'jp-r-2-passive', 'jp-r-3-subordinate', 'jp-u-1-negation',    'jp-u-2-invert', 'jp-u-3-past-tense', 'ng-sentence','ng-subordinate-sentence', 'ng-passive-sentence', 'ng-it','ng-it-r-1-null_subject', 'ng-it-r-2-passive', 'ng-it-r-3-subordinate','ng-it-u-1-negation', 'ng-it-u-2-invert', 'ng-it-u-3-gender','ng-jp-r-1-sov', 'ng-jp-r-2-passive', 'ng-jp-r-3-subordinate','ng-jp-u-1-negation', 'ng-jp-u-2-invert', 'ng-jp-u-3-past-tense']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "32947686-43a8-4bd9-ae23-6198d762ae25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_answer(text):\n",
    "    answer = text.split(\"A:\")[-1].strip()\n",
    "    return answer\n",
    "\n",
    "def construct_prompt(train_dataset, num_demonstrations):\n",
    "    assert num_demonstrations > 0\n",
    "    prompt = ''\n",
    "    train_examples = train_dataset.shuffle().select(range(num_demonstrations))\n",
    "    for exemplar_num in range(num_demonstrations):\n",
    "        train_example = train_examples[exemplar_num]\n",
    "        use_bad_sentence = random.choice([True, False])\n",
    "        exemplar = \"Q: Is this sentence grammatical? Yes or No: \"\n",
    "        if use_bad_sentence:\n",
    "            exemplar += train_example[\"ng-\" + col]\n",
    "            exemplar += \"\\nA: No\"\n",
    "        else:\n",
    "            exemplar += train_example[col]\n",
    "            exemplar += \"\\nA: Yes\"\n",
    "        exemplar += \"\\n\\n\"\n",
    "        prompt += exemplar\n",
    "    return prompt\n",
    "\n",
    "def compute_accuracy(preds, golds):\n",
    "    assert len(preds) == len(golds)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if pred == gold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8c72e903-5942-4227-b41a-478f267cb132",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_aligned_words_measures(text: str, \n",
    "                               measure: str,\n",
    "                               model: GPT2LMHeadModel, \n",
    "                               tokenizer: GPT2Tokenizer) -> list[str]:\n",
    "    \"\"\" Returns words and their measure (prob|surp)\n",
    "    Args:\n",
    "        text (list[str]): list of sentences\n",
    "        measure (str): Measure, either probability or surprisal\n",
    "                        (options: prob|surp)\n",
    "        model (GPT2LMHeadModel): Pretrained model\n",
    "        tokenizer (GPT2Tokenizer): Tokenizer\n",
    "    Returns:\n",
    "        list[str]: List of words with their measures\n",
    "\n",
    "    For example, \n",
    "    >>> model, tokenizer = load_pretrained_model()\n",
    "    >>> get_aligned_words_measures('the student is happy', \n",
    "    ...        'surp', model, tokenizer)\n",
    "    [('the', 0), ('student', 17.38616943359375), ('is', 6.385905742645264),\n",
    "     ('happy', 9.564245223999023)]\n",
    "    >>> get_aligned_words_measures('the cat is fluffy', \n",
    "    ...        'prob', model, tokenizer) \n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('is', 0.025296149775385857),\n",
    "     ('fluffy', 0.00020585735910572112)]\n",
    "    >>> get_aligned_words_measures('the cat are fluffy', \n",
    "    ...        'prob', model, tokenizer)\n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('are', 0.0010310395155102015),\n",
    "     ('fluffy', 0.00021902224398218095)]\n",
    "    \"\"\"\n",
    "    if measure not in {'prob', 'surp'}:\n",
    "        sys.stderr.write(f\"{measure} not recognized\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    ids = tokenizer(text, return_tensors='pt').to(device)\n",
    "    input_ids = ids.input_ids.flatten().data\n",
    "    target_ids = ids.input_ids[:,1:]\n",
    "\n",
    "    # get output\n",
    "    logits = model(**ids).logits\n",
    "    output = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "    if measure == 'surp':\n",
    "        output = -(output/torch.log(torch.tensor(2.0)))\n",
    "    else:\n",
    "        output = torch.exp(output)\n",
    "\n",
    "    # get by token measures \n",
    "    target_measures = output[:,:-1, :]\n",
    "    # use gather to get the output for each target item in the batch\n",
    "    target_measures = target_measures.gather(-1,\n",
    "                             target_ids.unsqueeze(2)).flatten().tolist()\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)[1:]\n",
    "    words = text.split(' ')\n",
    "\n",
    "    # for idx, token in enumerate(tokens):\n",
    "    #     if (not '▁' in token):\n",
    "    #         tokens[idx - 1] += token\n",
    "    #         del tokens[idx]\n",
    "    # print(tokens, words)\n",
    "    # A lil loop to force align words \n",
    "    current_word = words.pop(0)\n",
    "    current_token = tokens.pop(0).replace('▁', '')\n",
    "    measure = 0\n",
    "    while len(data) != len(text.split(' ')) and len(target_measures) > 0:\n",
    "        if current_word == current_token:\n",
    "            data.append((current_word, measure))\n",
    "            measure = 0\n",
    "            if words:\n",
    "                current_word = words.pop(0)\n",
    "                current_token = tokens.pop(0).replace('▁', '')\n",
    "                measure += target_measures.pop(0)\n",
    "        else:\n",
    "            measure += target_measures.pop(0)\n",
    "            current_token += tokens.pop(0).replace('▁', '')\n",
    "            data.append((current_token, measure))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0369f83b-fff4-4681-bdec-ae1c647bd94f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/arunas/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "2023-11-27 18:50:51.700579: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-27 18:50:51.702407: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 18:50:51.740019: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 18:50:53.520766: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> We will provide you a set of sentences which follow or violate a grammatical structure. \n",
      " The sentences may use subjects and objects from the following nouns - author, banana, biscuit, book, bottle, box, boy, bulb, cap, cat, chalk, chapter, cucumber, cup, dog, fish, fruit, girl, Gomu, Harry, hill, John, Leela, man, Maria, meal, mountain, mouse, newspaper, pear, pizza, poem, poet, rock, roof, Sheela, speaker, staircase, story, teacher, Tom, toy, tree, woman, writer.\n",
      "The sentences may use any of the following verbs - brings, carries, claims, climbs, eats, holds, notices, reads, says, sees, states, takes.\n",
      " Each noun in a sentence may sometimes use a different determiner than those found in English. Here is a reference of determiners that can be used by nouns: \"pear\": \"kar\", \"author\": \"kon\", \"authors\": \"kons\", \"banana\": \"kar\", \"biscuit\": \"kon\", \"book\": \"kon\", \"bottle\": \"kar\", \"box\": \"kar\", \"boy\": \"kon\", \"boys\": \"kons\", \"bulb\": \"kar\", \"cabinet\": \"kar\", \"cap\": \"kon\", \"cat\": \"kon\", \"cats\": \"kons\", \"chapter\": \"kon\", \"chalk\": \"kon\", \"cup\": \"kar\", \"cucumber\": \"kon\", \"dog\": \"kon\", \"dogs\": \"kons\", \"fish\": \"kon\", \"fruit\": \"kar\", \"girl\": \"kar\", \"girls\": \"kars\", \"hill\": \"kar\", \"man\": \"kon\", \"men\": \"kons\", \"meal\": \"kon\", \"mountain\": \"kar\", \"mouse\": \"kon\", \"newspaper\": \"kon\", \"pizza\": \"kar\", \"poet\": \"kon\", \"poets\": \"kons\", \"poem\": \"kar\", \"rock\": \"kon\", \"roof\": \"kon\", \"speaker\": \"kon\", \"speakers\": \"kons\", \"staircase\": \"kar\", \"story\": \"kar\", \"teacher\": \"kon\", \"teachers\": \"kons\", \"toy\": \"kon\", \"tree\": \"kar\", \"woman\": \"kar\", \"women\": \"kars\", \"writer\": \"kon\", \"writers\": \"kons\". Each verb in a sentence may sometimes use the past tense of the verb if it is more appropriate. Here are a set of verbs and their past tenses - \"climbs\" : \"climbed\", \"reads\": \"read\", \"carries\": \"carried\", \"eats\": \"ate\", \"holds\": \"held\", \"takes\" :\"took\", \"brings\": \"brought\", \"reads\": \"read\", \"climb\" : \"climbed\", \"read\": \"read\", \"carry\": \"carried\", \"eat\": \"ate\", \"hold\": \"held\", \"take\" :\"took\", \"bring\": \"brought\", \"read\": \"read\"\n",
      " The sentences may sometimes use the infinitive forms of a verb. Here are a set of verbs and their infinitives - \"climbs\" : \"to climb\", \"reads\": \"to read\", \"carries\": \"to carry\", \"eats\": \"to eat\", \"holds\": \"to hold\", \"takes\" : \"to take\", \"brings\": \"to bring\", \"reads\": \"to read\", \"climb\" : \"to climb\", \"read\": \"to read\", \"carry\": \"to carry\", \"eat\": \"to eat\", \"hold\": \"to hold\", \"take\" : \"to take\", \"bring\": \"to bring\", \"read\": \"to read\". \n",
      " The sentences may sometimes use the plural form of a noun. Here are a set of nouns and their plurals - \"fish\": \"fish\", \"mouse\": \"mice\", \"bottle\": \"bottles\", \"newspaper\": \"newspapers\", \"chalk\": \"chalks\", \"box\": \"boxes\", \"cap\": \"caps\", \"bulb\": \"bulbs\", \"cup\": \"cups\", \"toy\": \"toys\", \"staircase\": \"staircases\", \"rock\": \"rocks\", \"hill\": \"hills\", \"mountain\": \"mountains\", \"roof\": \"roofs\", \"tree\": \"trees\", \"biscuit\": \"biscuits\", \"banana\": \"bananas\", \"pear\": \"pears\", \"meal\": \"meals\", \"fruit\": \"fruits\", \"cucumber\": \"cucumbers\", \"pizza\": \"pizzas\", \"book\": \"books\", \"poem\": \"poems\", \"story\": \"stories\", \"chapter\": \"chapters\". \n",
      " The sentences may sometimes use the passive form of a verb. Here are a set of verbs and their passive forms - \"carries\": \"carried\", \"carry\": \"carried\", \"holds\": \"held\", \"hold\": \"held\", \"takes\": \"taken\", \"take\": \"taken\", \"brings\": \"brought\", \"bring\": \"brought\", \"climbs\": \"climbed\", \"climb\": \"climbed\", \"eats\": \"eaten\", \"eat\": \"eaten\", \"reads\": \"read\", \"read\": \"read\"\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the women bring a bottle\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the cats carry a box\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: man the holds the bulb\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the dog takes the cup\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: carry boys the a bottle\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: a teacher reads the poem\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the boys bring a mouse\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the the carries cat newspaper\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: the men take the chalk\n",
      "A: Yes\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: woman the takes the mouse\n",
      "A: No\n",
      "\n",
      "Q: Is this sentence grammatical? Yes or No: takes dog the a bulb\n",
      "A: Nootrop\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "f = pd.DataFrame(columns=[\"type\", \"prompt\", \"q\", \"prediction\", \"gold\", \"surprisal\", \"int-grad\"])\n",
    "f['type'] = 'test'\n",
    "g = pd.DataFrame(columns=['accuracy', 'type'])\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "datasets = {}\n",
    "for col in gCols:\n",
    "    datasets[col] = Dataset.from_pandas(pd.DataFrame(df[[col, 'ng-' + col]].copy())).train_test_split(test_size=0.2)\n",
    "\n",
    "master_prompt = 'We will provide you a set of sentences which follow or violate a grammatical structure. \\n The sentences may use subjects and objects from the following nouns - author, banana, biscuit, book, bottle, box, boy, bulb, cap, cat, chalk, chapter, cucumber, cup, dog, fish, fruit, girl, Gomu, Harry, hill, John, Leela, man, Maria, meal, mountain, mouse, newspaper, pear, pizza, poem, poet, rock, roof, Sheela, speaker, staircase, story, teacher, Tom, toy, tree, woman, writer.\\nThe sentences may use any of the following verbs - brings, carries, claims, climbs, eats, holds, notices, reads, says, sees, states, takes.\\n Each noun in a sentence may sometimes use a different determiner than those found in English. Here is a reference of determiners that can be used by nouns: \"pear\": \"kar\", \"author\": \"kon\", \"authors\": \"kons\", \"banana\": \"kar\", \"biscuit\": \"kon\", \"book\": \"kon\", \"bottle\": \"kar\", \"box\": \"kar\", \"boy\": \"kon\", \"boys\": \"kons\", \"bulb\": \"kar\", \"cabinet\": \"kar\", \"cap\": \"kon\", \"cat\": \"kon\", \"cats\": \"kons\", \"chapter\": \"kon\", \"chalk\": \"kon\", \"cup\": \"kar\", \"cucumber\": \"kon\", \"dog\": \"kon\", \"dogs\": \"kons\", \"fish\": \"kon\", \"fruit\": \"kar\", \"girl\": \"kar\", \"girls\": \"kars\", \"hill\": \"kar\", \"man\": \"kon\", \"men\": \"kons\", \"meal\": \"kon\", \"mountain\": \"kar\", \"mouse\": \"kon\", \"newspaper\": \"kon\", \"pizza\": \"kar\", \"poet\": \"kon\", \"poets\": \"kons\", \"poem\": \"kar\", \"rock\": \"kon\", \"roof\": \"kon\", \"speaker\": \"kon\", \"speakers\": \"kons\", \"staircase\": \"kar\", \"story\": \"kar\", \"teacher\": \"kon\", \"teachers\": \"kons\", \"toy\": \"kon\", \"tree\": \"kar\", \"woman\": \"kar\", \"women\": \"kars\", \"writer\": \"kon\", \"writers\": \"kons\". Each verb in a sentence may sometimes use the past tense of the verb if it is more appropriate. Here are a set of verbs and their past tenses - \"climbs\" : \"climbed\", \"reads\": \"read\", \"carries\": \"carried\", \"eats\": \"ate\", \"holds\": \"held\", \"takes\" :\"took\", \"brings\": \"brought\", \"reads\": \"read\", \"climb\" : \"climbed\", \"read\": \"read\", \"carry\": \"carried\", \"eat\": \"ate\", \"hold\": \"held\", \"take\" :\"took\", \"bring\": \"brought\", \"read\": \"read\"\\n The sentences may sometimes use the infinitive forms of a verb. Here are a set of verbs and their infinitives - \"climbs\" : \"to climb\", \"reads\": \"to read\", \"carries\": \"to carry\", \"eats\": \"to eat\", \"holds\": \"to hold\", \"takes\" : \"to take\", \"brings\": \"to bring\", \"reads\": \"to read\", \"climb\" : \"to climb\", \"read\": \"to read\", \"carry\": \"to carry\", \"eat\": \"to eat\", \"hold\": \"to hold\", \"take\" : \"to take\", \"bring\": \"to bring\", \"read\": \"to read\". \\n The sentences may sometimes use the plural form of a noun. Here are a set of nouns and their plurals - \"fish\": \"fish\", \"mouse\": \"mice\", \"bottle\": \"bottles\", \"newspaper\": \"newspapers\", \"chalk\": \"chalks\", \"box\": \"boxes\", \"cap\": \"caps\", \"bulb\": \"bulbs\", \"cup\": \"cups\", \"toy\": \"toys\", \"staircase\": \"staircases\", \"rock\": \"rocks\", \"hill\": \"hills\", \"mountain\": \"mountains\", \"roof\": \"roofs\", \"tree\": \"trees\", \"biscuit\": \"biscuits\", \"banana\": \"bananas\", \"pear\": \"pears\", \"meal\": \"meals\", \"fruit\": \"fruits\", \"cucumber\": \"cucumbers\", \"pizza\": \"pizzas\", \"book\": \"books\", \"poem\": \"poems\", \"story\": \"stories\", \"chapter\": \"chapters\". \\n The sentences may sometimes use the passive form of a verb. Here are a set of verbs and their passive forms - \"carries\": \"carried\", \"carry\": \"carried\", \"holds\": \"held\", \"hold\": \"held\", \"takes\": \"taken\", \"take\": \"taken\", \"brings\": \"brought\", \"bring\": \"brought\", \"climbs\": \"climbed\", \"climb\": \"climbed\", \"eats\": \"eaten\", \"eat\": \"eaten\", \"reads\": \"read\", \"read\": \"read\"\\n\\n'\n",
    "\n",
    "for NUM_DEMONSTRATIONS in range(10, 15, 5):\n",
    "    for col in gCols:\n",
    "        train_dataset = datasets[col]['train']\n",
    "        test_dataset = datasets[col]['test']\n",
    "        prompt = ''\n",
    "        printAnswer = True\n",
    "        \n",
    "        for test_sentence in train_dataset:\n",
    "            testBadOrGood = random.choice(['ng-', ''])\n",
    "            prompt = construct_prompt(train_dataset, NUM_DEMONSTRATIONS)\n",
    "            \n",
    "            fPrompt = prompt\n",
    "            \n",
    "            # Append test example\n",
    "            prompt += \"Q: Is this sentence grammatical? Yes or No: \"\n",
    "            prompt += test_sentence[testBadOrGood + col]\n",
    "            prompt += \"\\nA:\"\n",
    "            \n",
    "            fQ = \"Q: Is this sentence grammatical? Yes or No: \" + test_sentence[testBadOrGood + col] + \"\\nA:\"\n",
    "            \n",
    "            if testBadOrGood == 'ng-':\n",
    "                golds.append(\"No\")\n",
    "                fGold = 'No'\n",
    "            else:\n",
    "                golds.append(\"Yes\")\n",
    "                fGold = 'Yes'\n",
    "            \n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([master_prompt + prompt], return_tensors=\"pt\").to(device)\n",
    "            # answer = model.generate(prompt_tok,\n",
    "            #                     top_p=0.9, temperature=0.1,\n",
    "            #                     max_new_tokens=2)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=2, top_p=0.9, temperature=0.1, do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "            fPrediction = parse_answer(answer)\n",
    "            fSurprisal = get_aligned_words_measures(test_sentence[testBadOrGood + col] + \" \" + parse_answer(answer), \"surp\", model, tokenizer)\n",
    "            f = pd.concat([f, pd.DataFrame([{'type': col, 'prompt': fPrompt, 'q' :fQ, 'prediction': fPrediction, 'gold': fGold, 'surprisal': fSurprisal, 'int-grad': 0}])]).reset_index(drop=True)\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        g = pd.concat([g, pd.DataFrame([{ 'type' : col, 'accuracy': f\"{accuracy:.2f}\"}])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89fa9e7-5350-4304-acd8-458b6133a0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f.to_csv(\"llama-classification-train-test-det.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba0ab9c3-1584-488d-a711-5dbc3544c774",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m g \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassification-train-test-acc.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "g = pd.read_csv('llama-classification-train-test-acc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e8094-7af6-488d-b064-4ebba3347f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "golds = []\n",
    "\n",
    "gCols = [col for col in df.columns if not 'ng' in col]\n",
    "datasets = {}\n",
    "for col in gCols:\n",
    "    datasets[col] = Dataset.from_pandas(pd.DataFrame(df[[col, 'ng-' + col]].copy())).train_test_split(test_size=0.2)\n",
    "\n",
    "filename = \"classification-train-train.txt\"\n",
    "f = open(filename, \"a\")\n",
    "\n",
    "\n",
    "print(\"____________________CLASSIFICATION (TRAIN TRAIN)____________________\\n\")\n",
    "f.write(\"____________________CLASSIFICATION (TRAIN TRAIN)____________________\\n\")\n",
    "\n",
    "for NUM_DEMONSTRATIONS in range(10, 30, 5):\n",
    "    print(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    f.write(f\"____________________NUM DEMONSTRATIONS = {NUM_DEMONSTRATIONS}____________________\\n\")\n",
    "    for col in gCols:\n",
    "        train_dataset = datasets[col]['train']\n",
    "        test_dataset = datasets[col]['test']\n",
    "        prompt = ''\n",
    "        printAnswer = False\n",
    "        for test_sentence in train_dataset:\n",
    "            testBadOrGood = random.choice(['ng-', ''])\n",
    "            prompt = construct_prompt(train_dataset, NUM_DEMONSTRATIONS)\n",
    "            # Append test example\n",
    "            prompt += \"Q: Is this sentence grammatical? Yes or No: \"\n",
    "            prompt += test_sentence[testBadOrGood + col]\n",
    "            prompt += \"\\nA:\"\n",
    "            if testBadOrGood == 'ng-':\n",
    "                golds.append(\"No\")\n",
    "            else:\n",
    "                golds.append(\"Yes\")\n",
    "\n",
    "            # Get answer from model\n",
    "            model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(device)\n",
    "            # answer = model.generate(prompt_tok,\n",
    "            #                     top_p=0.9, temperature=0.1,\n",
    "            #                     max_new_tokens=2)\n",
    "            answer = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=2, do_sample=True)\n",
    "            answer = tokenizer.batch_decode(answer)[0]\n",
    "            if printAnswer:\n",
    "                print(answer)\n",
    "                printAnswer = False\n",
    "            preds.append(parse_answer(answer))\n",
    "\n",
    "        # Evaluate\n",
    "        accuracy = compute_accuracy(preds, golds)\n",
    "        print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7131c3-a693-4564-b972-105517c6be9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
