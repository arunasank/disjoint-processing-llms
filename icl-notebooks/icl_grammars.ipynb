{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7131c3-a693-4564-b972-105517c6be9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
      "0it [00:00, ?it/s]\n",
      "There was a problem when trying to write in your cache folder (/afs/csail.mit.edu/u/a/arunas/.cache/huggingface/hub). You should set the environment variable TRANSFORMERS_CACHE to a writable directory.\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:20<00:00,  1.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'': 1})\n",
      "['ita', 'ita-r-1-null_subject', 'ita-r-2-subordinate', 'ita-r-3-passive', 'ita-u-1-negation', 'ita-u-2-invert', 'ita-u-3-gender', 'en', 'en-r-1-subordinate', 'en-r-2-passive', 'en-u-1-negation', 'en-u-2-inversion', 'en-u-3-qsubordinate', 'it', 'it-r-1-null_subject', 'it-r-2-passive', 'it-r-3-subordinate', 'it-u-1-negation', 'it-u-2-invert', 'it-u-3-gender', 'jp-r-1-sov', 'jp-r-2-passive', 'jp-r-3-subordinate', 'jp-u-1-negation', 'jp-u-2-invert', 'jp-u-3-past-tense', 'jap-r-1-sov', 'jap-r-2-passive', 'jap-u-1-negation', 'jap-u-2-invert']\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, AutoConfig,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "import bitsandbytes\n",
    "from accelerate import infer_auto_device_map\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "import torch\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.environ['HF_TOKEN'] = \"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\"\n",
    "PREFIX = '/mnt/align4_drive/arunas'\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-70b-hf\", cache_dir='/mnt/align4_drive/arunas/llama-tensors/')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "           \"meta-llama/Llama-2-70b-hf\", config=config, cache_dir='/mnt/align4_drive/arunas/llama-tensors/', device_map=\"auto\", padding_side=\"left\"\n",
    "           )\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-70b-hf\",  config=config, quantization_config=nf4_config, device_map='auto', cache_dir='/mnt/align4_drive/arunas/llama-tensors/') # Load the model\n",
    "\n",
    "device_map = infer_auto_device_map(model)\n",
    "print(device_map)\n",
    "df = pd.read_csv(f'{PREFIX}/broca/data-gen/ngs.csv')\n",
    "cols = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a956dca-dc2a-4fc5-b40c-8d13318ddc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_answer(text):\n",
    "    answers = []\n",
    "    for t in text:\n",
    "        ans = t.split(\"A:\")[-1].strip()\n",
    "        answers.append(ans)\n",
    "    return answers\n",
    "\n",
    "def construct_prompt(train_dataset, num_demonstrations):\n",
    "    assert num_demonstrations > 0\n",
    "    prompt = ''\n",
    "    train_examples = train_dataset.shuffle().select(range(num_demonstrations))\n",
    "    for exemplar_num in range(num_demonstrations):\n",
    "        train_example = train_examples[exemplar_num]\n",
    "        use_bad_sentence = random.choice([True, False])\n",
    "        exemplar = \"Q: Is this sentence grammatical? Yes or No: \"\n",
    "        if use_bad_sentence:\n",
    "            exemplar += train_example[\"ng-\" + col]\n",
    "            exemplar += \"\\nA: No\"\n",
    "        else:\n",
    "            exemplar += train_example[col]\n",
    "            exemplar += \"\\nA: Yes\"\n",
    "        exemplar += \"\\n\\n\"\n",
    "        prompt += exemplar\n",
    "    return prompt\n",
    "\n",
    "def compute_accuracy(preds, golds):\n",
    "    assert len(preds) == len(golds)\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for pred, gold in zip(preds, golds):\n",
    "        if pred == gold:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    return correct / total\n",
    "\n",
    "import sys\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_aligned_words_measures(texts: str, \n",
    "                               answers: str,\n",
    "                               measure: str,\n",
    "                               model: GPT2LMHeadModel, \n",
    "                               tokenizer: GPT2Tokenizer) -> list[str]:\n",
    "    \"\"\" Returns words and their measure (prob|surp)\n",
    "    Args:\n",
    "        text (list[str]): list of sentences\n",
    "        measure (str): Measure, either probability or surprisal\n",
    "                        (options: prob|surp)\n",
    "        model (GPT2LMHeadModel): Pretrained model\n",
    "        tokenizer (GPT2Tokenizer): Tokenizer\n",
    "    Returns:\n",
    "        list[str]: List of words with their measures\n",
    "\n",
    "    For example, \n",
    "    >>> model, tokenizer = load_pretrained_model()\n",
    "    >>> get_aligned_words_measures('the student is happy', \n",
    "    ...        'surp', model, tokenizer)\n",
    "    [('the', 0), ('student', 17.38616943359375), ('is', 6.385905742645264),\n",
    "     ('happy', 9.564245223999023)]\n",
    "    >>> get_aligned_words_measures('the cat is fluffy', \n",
    "    ...        'prob', model, tokenizer) \n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('is', 0.025296149775385857),\n",
    "     ('fluffy', 0.00020585735910572112)]\n",
    "    >>> get_aligned_words_measures('the cat are fluffy', \n",
    "    ...        'prob', model, tokenizer)\n",
    "    [('the', 0), ('cat', 2.5601848392398097e-06), ('are', 0.0010310395155102015),\n",
    "     ('fluffy', 0.00021902224398218095)]\n",
    "    \"\"\"\n",
    "    print(texts, answers)\n",
    "    if measure not in {'prob', 'surp'}:\n",
    "        sys.stderr.write(f\"{measure} not recognized\\n\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    datas = []\n",
    "    for t in range(len(texts)):\n",
    "        text = f'{texts[t]} {answers[t]}'\n",
    "        data = []\n",
    "    \n",
    "        ids = tokenizer(text, return_tensors='pt')\n",
    "        input_ids = ids.input_ids.flatten().data\n",
    "        target_ids = ids.input_ids[:,1:]\n",
    "    \n",
    "        # get output\n",
    "        logits = model(**ids).logits\n",
    "        output = torch.nn.functional.log_softmax(logits, dim=-1)\n",
    "        if measure == 'surp':\n",
    "            output = -(output/torch.log(torch.tensor(2.0)))\n",
    "        else:\n",
    "            output = torch.exp(output)\n",
    "    \n",
    "        # get by token measures \n",
    "        target_measures = output[:,:-1, :]\n",
    "        # use gather to get the output for each target item in the batch\n",
    "        target_measures = target_measures.gather(-1,\n",
    "                                 target_ids.unsqueeze(2)).flatten().tolist()\n",
    "        \n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids)[1:]\n",
    "        words = text.split(' ')\n",
    "    \n",
    "        # A lil loop to force align words \n",
    "        current_word = words.pop(0)\n",
    "        current_token = tokens.pop(0).replace('▁', '')\n",
    "        measure = 0\n",
    "        while len(data) != len(text.split(' ')) and len(target_measures) > 0:\n",
    "            if current_word == current_token:\n",
    "                data.append((current_word, measure))\n",
    "                measure = 0\n",
    "                if words:\n",
    "                    current_word = words.pop(0)\n",
    "                    current_token = tokens.pop(0).replace('▁', '')\n",
    "                    measure += target_measures.pop(0)\n",
    "            else:\n",
    "                measure += target_measures.pop(0)\n",
    "                current_token += tokens.pop(0).replace('▁', '')\n",
    "                data.append((current_token, measure))\n",
    "        datas.append(data)\n",
    "    return datas\n",
    "\n",
    "preds = []\n",
    "golds = []\n",
    "\n",
    "f = pd.DataFrame(columns=[\"type\", \"prompt\", \"q\", \"prediction\", \"gold\", \"surprisal\", \"int-grad\"])\n",
    "f['type'] = 'test'\n",
    "g = pd.DataFrame(columns=['accuracy', 'type'])\n",
    "\n",
    "gCols = [col for col in list(df.columns) if not 'ng' in col]\n",
    "print(gCols)\n",
    "datasets = {}\n",
    "for col in gCols:\n",
    "    datasets[col] = Dataset.from_pandas(pd.DataFrame(df[[col, 'ng-' + col]].copy())).train_test_split(test_size=0.5)\n",
    "\n",
    "master_prompt = 'We will provide you a set of sentences which follow or violate a grammatical structure. \\n The sentences may use subjects and objects from the following nouns - author, banana, biscuit, book, bottle, box, boy, bulb, cap, cat, chalk, chapter, cucumber, cup, dog, fish, fruit, girl, Gomu, Harry, hill, John, Leela, man, Maria, meal, mountain, mouse, newspaper, pear, pizza, poem, poet, rock, roof, Sheela, speaker, staircase, story, teacher, Tom, toy, tree, woman, writer.\\nThe sentences may use any of the following verbs - brings, carries, claims, climbs, eats, holds, notices, reads, says, sees, states, takes.\\n Each noun in a sentence may sometimes use a different determiner than those found in English. Here is a reference of determiners that can be used by nouns: \"pear\": \"kar\", \"author\": \"kon\", \"authors\": \"kons\", \"banana\": \"kar\", \"biscuit\": \"kon\", \"book\": \"kon\", \"bottle\": \"kar\", \"box\": \"kar\", \"boy\": \"kon\", \"boys\": \"kons\", \"bulb\": \"kar\", \"cabinet\": \"kar\", \"cap\": \"kon\", \"cat\": \"kon\", \"cats\": \"kons\", \"chapter\": \"kon\", \"chalk\": \"kon\", \"cup\": \"kar\", \"cucumber\": \"kon\", \"dog\": \"kon\", \"dogs\": \"kons\", \"fish\": \"kon\", \"fruit\": \"kar\", \"girl\": \"kar\", \"girls\": \"kars\", \"hill\": \"kar\", \"man\": \"kon\", \"men\": \"kons\", \"meal\": \"kon\", \"mountain\": \"kar\", \"mouse\": \"kon\", \"newspaper\": \"kon\", \"pizza\": \"kar\", \"poet\": \"kon\", \"poets\": \"kons\", \"poem\": \"kar\", \"rock\": \"kon\", \"roof\": \"kon\", \"speaker\": \"kon\", \"speakers\": \"kons\", \"staircase\": \"kar\", \"story\": \"kar\", \"teacher\": \"kon\", \"teachers\": \"kons\", \"toy\": \"kon\", \"tree\": \"kar\", \"woman\": \"kar\", \"women\": \"kars\", \"writer\": \"kon\", \"writers\": \"kons\". Each verb in a sentence may sometimes use the past tense of the verb if it is more appropriate. Here are a set of verbs and their past tenses - \"climbs\" : \"climbed\", \"reads\": \"read\", \"carries\": \"carried\", \"eats\": \"ate\", \"holds\": \"held\", \"takes\" :\"took\", \"brings\": \"brought\", \"reads\": \"read\", \"climb\" : \"climbed\", \"read\": \"read\", \"carry\": \"carried\", \"eat\": \"ate\", \"hold\": \"held\", \"take\" :\"took\", \"bring\": \"brought\", \"read\": \"read\"\\n The sentences may sometimes use the infinitive forms of a verb. Here are a set of verbs and their infinitives - \"climbs\" : \"to climb\", \"reads\": \"to read\", \"carries\": \"to carry\", \"eats\": \"to eat\", \"holds\": \"to hold\", \"takes\" : \"to take\", \"brings\": \"to bring\", \"reads\": \"to read\", \"climb\" : \"to climb\", \"read\": \"to read\", \"carry\": \"to carry\", \"eat\": \"to eat\", \"hold\": \"to hold\", \"take\" : \"to take\", \"bring\": \"to bring\", \"read\": \"to read\". \\n The sentences may sometimes use the plural form of a noun. Here are a set of nouns and their plurals - \"fish\": \"fish\", \"mouse\": \"mice\", \"bottle\": \"bottles\", \"newspaper\": \"newspapers\", \"chalk\": \"chalks\", \"box\": \"boxes\", \"cap\": \"caps\", \"bulb\": \"bulbs\", \"cup\": \"cups\", \"toy\": \"toys\", \"staircase\": \"staircases\", \"rock\": \"rocks\", \"hill\": \"hills\", \"mountain\": \"mountains\", \"roof\": \"roofs\", \"tree\": \"trees\", \"biscuit\": \"biscuits\", \"banana\": \"bananas\", \"pear\": \"pears\", \"meal\": \"meals\", \"fruit\": \"fruits\", \"cucumber\": \"cucumbers\", \"pizza\": \"pizzas\", \"book\": \"books\", \"poem\": \"poems\", \"story\": \"stories\", \"chapter\": \"chapters\". \\n The sentences may sometimes use the passive form of a verb. Here are a set of verbs and their passive forms - \"carries\": \"carried\", \"carry\": \"carried\", \"holds\": \"held\", \"hold\": \"held\", \"takes\": \"taken\", \"take\": \"taken\", \"brings\": \"brought\", \"bring\": \"brought\", \"climbs\": \"climbed\", \"climb\": \"climbed\", \"eats\": \"eaten\", \"eat\": \"eaten\", \"reads\": \"read\", \"read\": \"read\"\\n\\n'\n",
    "NUM_DEMONSTRATIONS = 2\n",
    "BATCH_SIZE = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0201da1b-a496-4f2b-b7f2-3ac36bb9fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['una ragazza prende cappello un', 'le ragazze portano il giornale', 'un ragazzo mangia pesce un', 'un ragazzo prende bottiglia una'] ['No', 'Yes', 'No', 'No']\n",
      "['una ragazza prende cappello un', 'le ragazze portano il giornale', 'un ragazzo mangia pesce un', 'un ragazzo prende bottiglia una'] ['No', 'Yes', 'No', 'No']\n",
      "[[('una', 0), ('ragaz', 30.9542293548584), ('ragazza', 32.31256902217865), ('ragazza', 32.31256902217865), ('prende', 11.261316452175379), ('prende', 11.261316452175379)], [('le', 0), ('ragaz', 5.509597212949302e-05), ('ragazze', 0.11016257000119367), ('ragazze', 0.11016257000119367), ('portano', 0.9785170019604266), ('portano', 0.9785170019604266)], [('un', 0), ('ragazzo', 9.764993069438788e-05), ('ragazzo', 9.764993069438788e-05), ('mangia', 0.9024602283534477), ('mangia', 0.9024602283534477), ('pesce', 0.7793480681721121)], [('un', 0), ('ragazzo', 9.764993069438788e-05), ('ragazzo', 9.764993069438788e-05), ('prende', 0.9029960754560307), ('prende', 0.9029960754560307), ('bottiglia', 0.5306668026132684)]]\n",
      "['i ragazzi portano il gesso', 'la ragazza legge il libro', \"l'autore legge il poema\", 'i cani portano il giornale'] ['No', 'Yes', 'Yes', 'Yes']\n",
      "['i ragazzi portano il gesso', 'la ragazza legge il libro', \"l'autore legge il poema\", 'i cani portano il giornale'] ['No', 'Yes', 'Yes', 'Yes']\n",
      "[[('i', 0), ('ragaz', 32.81043338775635), ('ragazzi', 33.09551241993904), ('ragazzi', 33.09551241993904), ('portano', 12.080526401638053), ('portano', 12.080526401638053)], [('la', 0), ('ragaz', 6.818677866249345e-05), ('ragazza', 0.6147328605838993), ('ragazza', 0.6147328605838993), ('legge', 0.9936925749843795), ('legge', 0.9936925749843795)], [(\"l'\", 0.00015275934129022062), (\"l'autore\", 0.012899895285954699), (\"l'autore\", 0.012899895285954699), ('legge', 0.0008482543053105474), ('legge', 0.0008482543053105474)], [('i', 0), ('cani', 0.005194050929276273), ('cani', 0.005194050929276273), ('portano', 0.001730163074171287), ('portano', 0.001730163074171287), ('il', 0.6762041449546814)]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m         f \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([f, pd\u001b[38;5;241m.\u001b[39mDataFrame([{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m'\u001b[39m: col, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m'\u001b[39m: fPrompts[batch_idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m :fQs[batch_idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m: fPredictions[batch_idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold\u001b[39m\u001b[38;5;124m'\u001b[39m: fGolds[batch_idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurprisal\u001b[39m\u001b[38;5;124m'\u001b[39m: fSurprisals[batch_idx], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint-grad\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m}])])\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m compute_accuracy(preds, golds)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -- Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m g \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([g, pd\u001b[38;5;241m.\u001b[39mDataFrame([{ \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrainType\u001b[39m\u001b[38;5;124m'\u001b[39m : col, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtestType\u001b[39m\u001b[38;5;124m'\u001b[39m: col, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}])])\n",
      "Cell \u001b[0;32mIn[1], line 62\u001b[0m, in \u001b[0;36mcompute_accuracy\u001b[0;34m(preds, golds)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_accuracy\u001b[39m(preds, golds):\n\u001b[0;32m---> 62\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(preds) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(golds)\n\u001b[1;32m     63\u001b[0m     total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     64\u001b[0m     correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in gCols[:1]:\n",
    "    train_dataset = datasets[col]['train'].select(list(range(8)))\n",
    "    test_dataset = datasets[col]['test'].select(list(range(8)))\n",
    "    printAnswer = False\n",
    "    f = pd.DataFrame(columns=[\"type\", \"prompt\", \"q\", \"prediction\", \"gold\", \"surprisal\", \"int-grad\"])\n",
    "    for i in range(0, len(test_dataset), BATCH_SIZE):\n",
    "        test_sentences = []\n",
    "        fPrompts = []\n",
    "        fQs = []\n",
    "        fGolds = []\n",
    "        prompts = []\n",
    "        for batch_idx in range(BATCH_SIZE):\n",
    "            #print(f'batch_idx: {i + batch_idx} {len(test_dataset)}')\n",
    "            testBadOrGood = random.choice(['ng-', ''])\n",
    "            if (i + batch_idx) >= len(test_dataset):\n",
    "                break;\n",
    "            test_sentence = test_dataset[i + batch_idx]\n",
    "            prompt = construct_prompt(train_dataset, NUM_DEMONSTRATIONS)\n",
    "            \n",
    "            fPrompt = prompt\n",
    "            \n",
    "            # Append test example\n",
    "            prompt += \"Q: Is this sentence grammatical? Yes or No: \"\n",
    "            prompt += test_sentence[testBadOrGood + col]\n",
    "            prompt += \"\\nA:\"\n",
    "            \n",
    "            fQ = \"Q: Is this sentence grammatical? Yes or No: \" + test_sentence[testBadOrGood + col] + \"\\nA:\"\n",
    "            \n",
    "            if testBadOrGood == 'ng-':\n",
    "                golds.append(\"No\")\n",
    "                fGold = 'No'\n",
    "            else:\n",
    "                golds.append(\"Yes\")\n",
    "                fGold = 'Yes'\n",
    "            fGolds.append(fGold)\n",
    "            prompts.append(master_prompt + prompt)\n",
    "            test_sentences.append(test_sentence[testBadOrGood + col])\n",
    "            fPrompts.append(fPrompt)\n",
    "            fQs.append(fQ)\n",
    "            \n",
    "        # Get answer from model\n",
    "        model_inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True).to('cuda')\n",
    "        answers = model.generate(**model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=2, top_p=0.9, temperature=0.1, do_sample=True)\n",
    "        answers = tokenizer.batch_decode(answers)[:BATCH_SIZE]\n",
    "\n",
    "        if printAnswer:\n",
    "            print(answers)\n",
    "            printAnswer = False\n",
    "        \n",
    "        preds = preds + parse_answer(answers)\n",
    "        fPredictions = parse_answer(answers)\n",
    "        print(test_sentences, parse_answer(answers))\n",
    "        fSurprisals = get_aligned_words_measures(test_sentences, parse_answer(answers), \"surp\", model, tokenizer)\n",
    "        print(fSurprisals)\n",
    "        for batch_idx in range(len(fPrompts)):\n",
    "            f = pd.concat([f, pd.DataFrame([{'type': col, 'prompt': fPrompts[batch_idx], 'q' :fQs[batch_idx], 'prediction': fPredictions[batch_idx], 'gold': fGolds[batch_idx], 'surprisal': fSurprisals[batch_idx], 'int-grad': 0}])]).reset_index(drop=True)\n",
    "    # Evaluate\n",
    "    accuracy = compute_accuracy(preds, golds)\n",
    "    print(f\"{col} -- Accuracy: {accuracy:.2f}\\n\")\n",
    "    g = pd.concat([g, pd.DataFrame([{ 'trainType' : col, 'testType': col, 'accuracy': f\"{accuracy:.2f}\"}])])\n",
    "    # f.to_csv(f\"{PREFIX}/broca/llama/experiments/llama-classification-train-test-det-{col}.csv\")\n",
    "\n",
    "# g.to_csv(f'{PREFIX}/broca/llama/experiments/llama-classification-train-test-acc.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b458328-e64e-4b96-9288-b15f290db5ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [(una, 0), (ragaz, 30.9542293548584), (ragazza...\n",
       "1    [(le, 0), (ragaz, 5.509597212949302e-05), (rag...\n",
       "2    [(un, 0), (ragazzo, 9.764993069438788e-05), (r...\n",
       "3    [(un, 0), (ragazzo, 9.764993069438788e-05), (r...\n",
       "4    [(i, 0), (ragaz, 32.81043338775635), (ragazzi,...\n",
       "5    [(la, 0), (ragaz, 6.818677866249345e-05), (rag...\n",
       "6    [(l', 0.00015275934129022062), (l'autore, 0.01...\n",
       "7    [(i, 0), (cani, 0.005194050929276273), (cani, ...\n",
       "Name: surprisal, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f['surprisal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fe5a5d-1c1a-412f-bcb9-8d9746eafda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
