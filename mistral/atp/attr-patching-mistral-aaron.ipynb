{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a900da82-e783-4757-b886-8c82bcb90924",
   "metadata": {},
   "source": [
    "## Mistral Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6e939-c1ca-44b6-a6a3-d3a0a35c77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "from accelerate import infer_auto_device_map\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e608719b-53be-4dc8-b76e-d6a85812b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4ea41d3b17646409505962c773bfcec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = LanguageModel(\"/home/gridsan/arunas/models/mistralai/Mistral-7B-v0.1/\",  load_in_8bit=True, dispatch=True, device_map='auto') # Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cd983e9-557f-4589-830a-cc15d5b738b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ita', 'ita-r-1-null_subject', 'ita-r-2-subordinate', 'ita-r-3-passive',\n",
       "       'ita-u-1-negation', 'ita-u-2-invert', 'ita-u-3-gender', 'en',\n",
       "       'en-r-1-subordinate', 'en-r-2-passive', 'en-u-1-negation',\n",
       "       'en-u-2-inversion', 'en-u-3-qsubordinate', 'en-u-4-wh', 'it',\n",
       "       'it-r-1-null_subject', 'it-r-2-passive', 'it-r-3-subordinate',\n",
       "       'it-u-1-negation', 'it-u-2-invert', 'it-u-3-gender', 'jp-r-1-sov',\n",
       "       'jap-r-1-sov', 'jp-r-2-passive', 'jap-r-2-passive',\n",
       "       'jp-r-3-subordinate', 'jp-u-1-negation', 'jap-u-1-negation',\n",
       "       'jp-u-2-invert', 'jap-u-2-invert', 'jp-u-3-past-tense', 'ng-ita',\n",
       "       'ng-ita-r-1-null_subject', 'ng-ita-r-2-subordinate',\n",
       "       'ng-ita-r-3-passive', 'ng-ita-u-1-negation', 'ng-ita-u-2-invert',\n",
       "       'ng-ita-u-3-gender', 'ng-en', 'ng-en-r-1-subordinate',\n",
       "       'ng-en-r-2-passive', 'ng-en-u-1-negation', 'ng-en-u-2-inversion',\n",
       "       'ng-en-u-3-qsubordinate', 'ng-en-u-4-wh', 'ng-it',\n",
       "       'ng-it-r-1-null_subject', 'ng-it-r-2-passive', 'ng-it-r-3-subordinate',\n",
       "       'ng-it-u-1-negation', 'ng-it-u-2-invert', 'ng-it-u-3-gender',\n",
       "       'ng-jp-r-1-sov', 'ng-jap-r-1-sov', 'ng-jp-r-2-passive',\n",
       "       'ng-jap-r-2-passive', 'ng-jp-r-3-subordinate', 'ng-jp-u-1-negation',\n",
       "       'ng-jap-u-1-negation', 'ng-jp-u-2-invert', 'ng-jap-u-2-invert',\n",
       "       'ng-jp-u-3-past-tense'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og = pd.read_csv('/home/gridsan/arunas/broca/data-gen/ngs.csv')\n",
    "og.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "837b3d1c-abdc-4fa6-8056-b6f674cb2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_from_df(filename):\n",
    "    data = list(pd.read_csv(filename)['prompt'])\n",
    "    data = [sentence.strip() for sentence in data]\n",
    "    data = [sentence for sentence in data if not sentence == '']\n",
    "    data = [sentence.replace('</s>', '\\n') for sentence in data]\n",
    "    golds = [sentence.strip().split(\"\\n\")[-1].strip().split('A:')[-1].strip() for sentence in data]\n",
    "    data = [sentence[: -len(golds[idx])].strip() for idx, sentence in enumerate(data)]\n",
    "    return data, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14aeca53-a1db-4a37-802b-8a370db40233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sType=sys.argv[1]\n",
    "sType='en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "33b90b2b-200d-43c7-a493-1a673b70153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size))\n",
    "attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size))\n",
    "\n",
    "def attrPatching(fullPrompt, gold):\n",
    "    attn_layer_cache_prompt = {}\n",
    "    mlp_layer_cache_prompt = {}\n",
    "    \n",
    "    attn_layer_cache_patch = {}\n",
    "    mlp_layer_cache_patch = {}\n",
    "    if (gold == 'Yes'):\n",
    "        predictionExample = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        patch = og[og[sType] == predictionExample][f\"ng-{sType}\"].iloc[0]\n",
    "        patchPrompt = fullPrompt.replace(predictionExample, patch)\n",
    "    else:\n",
    "        patchPrompt = fullPrompt\n",
    "        patch = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        predictionExample = og[og[f\"ng-{sType}\"] == patch][sType].iloc[0]\n",
    "        fullPrompt = patchPrompt.replace(patch, predictionExample)\n",
    "        gold = \"Yes\"\n",
    "\n",
    "    notGold = \"No\"\n",
    "    gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "    notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     model.model.layers[31].mlp.down_proj.weight[1239] = torch.zeros_like(model.model.layers[31].mlp.down_proj.weight[1239])\n",
    "\n",
    "    with model.forward(inference=False) as runner:\n",
    "        print(model.model.layers[31].self_attn.o_proj.weight[1239].shape)\n",
    "        with runner.invoke(fullPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "    \n",
    "                attn_layer_cache_prompt[layer] = {\"forward\": self_attn.detach().save(), \"backward\": self_attn.grad.detach().save()}\n",
    "                mlp_layer_cache_prompt[layer] = {\"forward\": mlp.detach().save(), \"backward\": mlp.grad.detach().save()}\n",
    "            \n",
    "            logits = model.lm_head.output[:, -1, notGold] - model.lm_head.output[:, -1, gold]\n",
    "            loss = logits.sum()\n",
    "            loss.backward(retain_graph=False)\n",
    "    \n",
    "    with model.forward(inference=False) as runner:\n",
    "        with runner.invoke(patchPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "\n",
    "                attn_layer_cache_patch[layer] = {\"forward\": self_attn.detach().save()}\n",
    "                mlp_layer_cache_patch[layer] = {\"forward\": mlp.detach().save()}\n",
    "    \n",
    "    for layer in range(len(model.model.layers)):\n",
    "        # print(attn_layer_cache_patch[layer]['forward'].shape)\n",
    "        mlp_effects = mlp_layer_cache_prompt[layer][\"backward\"].value * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)\n",
    "        attn_effects = attn_layer_cache_prompt[layer][\"backward\"].value * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)\n",
    "\n",
    "        mlp_effects = mlp_effects[:, -1, :] # batch, token, hidden_states\n",
    "        attn_effects = attn_effects[:, -1, :] # batch, token, hidden_states\n",
    "\n",
    "        mlp_effects_cache[layer] += mlp_effects[0].cpu()\n",
    "        attn_effects_cache[layer] += attn_effects[0].cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7f45bfe4-996e-4a39-a421-2adb2e824a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4096])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:18, 18.54s/it]\n"
     ]
    }
   ],
   "source": [
    "prompts, golds = get_prompt_from_df(f'/home/gridsan/arunas/broca/llama/experiments/llama-classification-new-prompt-det-{sType}.csv')\n",
    "for prompt,gold in tqdm(zip(prompts[:1], golds[:1])):\n",
    "    attrPatching(prompt, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8764bd4b-3d62-42f0-96de-d05d71dfc2b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4096])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_effects_cache /= len(prompts)\n",
    "attn_effects_cache /= len(prompts)\n",
    "mlp_effects_cache.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8879b0c2-9445-40e9-be46-302661bf8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.nan_to_num(mlp_effects_cache)\n",
    "attn_effects_cache = torch.nan_to_num(attn_effects_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "55840345-1757-46ed-801c-8574977ef11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0168)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_effects_cache[31][1239]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f18a29c1-de03-48b2-8f93-7d07911e43bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131072\n"
     ]
    }
   ],
   "source": [
    "flattened_effects_cache = mlp_effects_cache.view(-1)\n",
    "print(flattened_effects_cache.shape[-1])\n",
    "top_neurons = flattened_effects_cache.topk(k=int((0.01 * flattened_effects_cache.shape[-1])))\n",
    "two_d_indices = torch.cat((((top_neurons[1] // mlp_effects_cache.shape[1]).unsqueeze(1)), ((top_neurons[1] % mlp_effects_cache.shape[1]).unsqueeze(1))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b1721-811b-4025-a9a9-23cfb6ac8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'mlp/new-prompt-{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(two_d_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abc286bc-4cd3-4bad-b302-37a5f072305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_effects_cache = attn_effects_cache.view(-1)\n",
    "top_neurons = flattened_effects_cache.topk(k=40)\n",
    "two_d_indices = torch.cat((((top_neurons[1] // attn_effects_cache.shape[1]).unsqueeze(1)), ((top_neurons[1] % attn_effects_cache.shape[1]).unsqueeze(1))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "96792c01-37cf-484d-b597-b98ccaa127f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'attn/new-prompt-{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(two_d_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6efd3a5f-7e6a-4526-87b5-ce20937ce068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce97ed4d-5d82-4e71-8f99-e0db9e60c926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor(..., device='meta', size=(4096, 4096), requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = model.model.layers[0].self_attn.o_proj.weight\n",
    "a\n",
    "# for layer_idx in range(num_layers):\n",
    "#     attention_layer = model.model.layers[layer_idx].self_attn\n",
    "#     print(attention_layer.num_heads, attention_layer.o_proj.out_features)\n",
    "#     num_heads = attention_layer.o_proj.out_features // attention_layer.num_heads\n",
    "#     print(f\"Layer {layer_idx}: {num_heads} attention heads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dae6879-8bb1-4ae3-8ab3-bdbb4164c135",
   "metadata": {},
   "source": [
    "### Delete this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de1278fb-eaf1-4217-82dc-4235a9799213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"mistral\",\n",
      "    \"model_path\": \"/home/gridsan/arunas/models/mistralai/Mistral-7B-v0.1\",\n",
      "    \"prefix\": \"/home/gridsan/arunas/\",\n",
      "    \"data_path\": \"/home/gridsan/arunas/broca/data-gen/ngs.csv\",\n",
      "    \"prompt_files_path\": \"/home/gridsan/arunas/broca/mistral/experiments/new-prompt/\",\n",
      "    \"patch_pickles_path\": \"/home/gridsan/arunas/broca/mistral/atp/patches/\",\n",
      "    \"patch_pickles_sub_path\": \"all-neurons\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--config_file', type=str, help='path to the model training config file, found in broca/config')\n",
    "parser.add_argument('--stype', type=int, help='structure type idx. Can range from 0-30')\n",
    "\n",
    "args = { \"config_file\": \"/home/gridsan/arunas/broca/configs/mistral-atp-config\", \"stype\": 7}\n",
    "with open(args['config_file'], 'r') as f:\n",
    "    config_file = yaml.safe_load(f)\n",
    "\n",
    "print(json.dumps(config_file, indent=4))\n",
    "PREFIX = config_file[\"prefix\"]\n",
    "MODEL_NAME = config_file[\"model_name\"]\n",
    "MODEL_PATH = config_file[\"model_path\"]\n",
    "DATA_PATH = config_file[\"data_path\"]\n",
    "PROMPT_FILES_PATH = config_file[\"prompt_files_path\"]\n",
    "PATCH_PICKLES_PATH = config_file[\"patch_pickles_path\"]\n",
    "PATCH_PICKLES_SUBPATH = config_file[\"patch_pickles_sub_path\"]\n",
    "\n",
    "if (MODEL_NAME == \"llama\"):\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(MODEL_PATH, cache_dir=MODEL_CACHE_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\", cache_dir=MODEL_CACHE_PATH)\n",
    "    \n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = LanguageModel(MODEL_PATH,  quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto', cache_dir=MODEL_CACHE_PATH) # Load the model\n",
    "elif (MODEL_NAME == \"mistral\"):\n",
    "    nf4_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = LanguageModel(MODEL_PATH,  quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto') # Load the model\n",
    "    \n",
    "model.requires_grad_(True)\n",
    "og = pd.read_csv(DATA_PATH)\n",
    "types = [col for col in og.columns if not 'ng-' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9f0de4a-4fbe-4a1c-9cb5-e3bfeb88bdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_from_df(filename):\n",
    "    data = list(pd.read_csv(filename)['prompt'])\n",
    "    data = [sentence.strip() for sentence in data]\n",
    "    data = [sentence for sentence in data if not sentence == '']\n",
    "    data = [sentence.replace('</s>', '\\n') for sentence in data]\n",
    "    golds = [sentence.strip().split(\"\\n\")[-1].strip().split('A:')[-1].strip() for sentence in data]\n",
    "    data = [sentence[: -len(golds[idx])].strip() for idx, sentence in enumerate(data)]\n",
    "    return data, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea54dde6-0f1e-4f7d-9200-b272774f1ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:45,  9.63s/it]\n"
     ]
    }
   ],
   "source": [
    "sType = types[args['stype']]\n",
    "\n",
    "mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "\n",
    "def attrPatching(fullPrompt, gold, idx):\n",
    "    attn_layer_cache_prompt = {}\n",
    "    mlp_layer_cache_prompt = {}\n",
    "\n",
    "    attn_layer_cache_patch = {}\n",
    "    mlp_layer_cache_patch = {}\n",
    "\n",
    "    if gold == 'Yes':\n",
    "        predictionExample = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        patch = og.iloc[idx][f\"ng-{sType}\"]\n",
    "        patchPrompt = fullPrompt.replace(predictionExample, patch)\n",
    "    else:\n",
    "        patchPrompt = fullPrompt\n",
    "        patch = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        predictionExample = og.iloc[idx][sType]\n",
    "        fullPrompt = patchPrompt.replace(patch, predictionExample)\n",
    "        gold = \"Yes\"\n",
    "\n",
    "    if model.tokenizer(fullPrompt, return_tensors=\"pt\").input_ids.shape[-1] != \\\n",
    "        model.tokenizer(patchPrompt, return_tensors=\"pt\").input_ids.shape[-1]:\n",
    "        return\n",
    "\n",
    "    notGold = \"No\"\n",
    "    gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "    notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "    with model.forward(inference=False) as runner:\n",
    "        with runner.invoke(fullPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "                mlp.retain_grad()\n",
    "                self_attn.retain_grad()\n",
    "    \n",
    "                attn_layer_cache_prompt[layer] = {\"forward\": self_attn.save()} # \"backward\": self_attn.grad.detach().save()}\n",
    "                mlp_layer_cache_prompt[layer] = {\"forward\": mlp.save()}# \"backward\": mlp.grad.detach().save()}\n",
    "    \n",
    "        logits = model.lm_head.output.save()\n",
    "    loss = logits.value[:, -1, notGold] - logits.value[:, -1, gold]\n",
    "    loss = loss.sum()\n",
    "    loss.backward()\n",
    "\n",
    "    with model.forward(inference=False) as runner:\n",
    "        with runner.invoke(patchPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "    \n",
    "                attn_layer_cache_patch[layer] = {\"forward\": self_attn.save()}\n",
    "                mlp_layer_cache_patch[layer] = {\"forward\": mlp.save()}\n",
    "\n",
    "    for layer in range(len(model.model.layers)):\n",
    "        mlp_effects = (mlp_layer_cache_prompt[layer][\"forward\"].value.grad * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "        attn_effects = (attn_layer_cache_prompt[layer][\"forward\"].value.grad * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "\n",
    "        mlp_effects = mlp_effects[0, -1, :] # batch, token, hidden_states\n",
    "        attn_effects = attn_effects[0, -1, :] # batch, token, hidden_states\n",
    "\n",
    "        mlp_effects_cache[layer] += mlp_effects\n",
    "        attn_effects_cache[layer] += attn_effects\n",
    "\n",
    "prompts, golds = get_prompt_from_df(f'{PROMPT_FILES_PATH}/{sType}.csv')\n",
    "for idx,(prompt,gold) in tqdm(enumerate(zip(prompts, golds))):\n",
    "    attrPatching(prompt, gold, idx)\n",
    "    if idx > 10:\n",
    "        break\n",
    "\n",
    "mlp_effects_cache /= len(prompts)\n",
    "attn_effects_cache /= len(prompts)\n",
    "\n",
    "with open(f'{PATCH_PICKLES_PATH}/mlp/{PATCH_PICKLES_SUBPATH}/{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(mlp_effects_cache, f)\n",
    "\n",
    "with open(f'{PATCH_PICKLES_PATH}/attn/{PATCH_PICKLES_SUBPATH}{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(attn_effects_cache, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5bd0c0f-f03f-4802-9d00-c3921c90ec8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"model_name\": \"mistral\",\n",
      "    \"model_path\": \"mistralai/Mistral-7B-v0.1\",\n",
      "    \"prefix\": \"/mnt/align4_drive/arunas/\",\n",
      "    \"data_path\": \"/mnt/align4_drive/arunas/broca/data-gen/ngs.csv\",\n",
      "    \"prompt_files_path\": \"/mnt/align4_drive/arunas/broca/mistral/experiments/new-prompt-prologue-random-seed/\",\n",
      "    \"patch_pickles_path\": \"/mnt/align4_drive/arunas/broca/mistral/atp/patches/\",\n",
      "    \"patch_pickles_sub_path\": \"all-neurons-new-prompt-prologue-random-seed\"\n",
      "}\n",
      "Running for ita\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a05d3a5cc8d4a7f9e1bf182056d850b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "15it [00:31,  2.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 133\u001b[0m\n\u001b[1;32m    131\u001b[0m prompts, questions, golds \u001b[38;5;241m=\u001b[39m get_prompt_from_df(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mPROMPT_FILES_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msType\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx,(prompt, q, gold) \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(prompts,questions, golds))):\n\u001b[0;32m--> 133\u001b[0m     attrPatching(prompt, q, gold, idx)\n\u001b[1;32m    135\u001b[0m mlp_effects_cache \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n\u001b[1;32m    136\u001b[0m attn_effects_cache \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(prompts)\n",
      "Cell \u001b[0;32mIn[4], line 109\u001b[0m, in \u001b[0;36mattrPatching\u001b[0;34m(cleanPrompt, q, gold, idx)\u001b[0m\n\u001b[1;32m    106\u001b[0m         mlp_layer_cache_prompt[layer] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward\u001b[39m\u001b[38;5;124m\"\u001b[39m: mlp\u001b[38;5;241m.\u001b[39msave()}\u001b[38;5;66;03m# \"backward\": mlp.grad.detach().save()}\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlm_head\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m--> 109\u001b[0m loss \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mvalue[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, notGold] \u001b[38;5;241m-\u001b[39m logits\u001b[38;5;241m.\u001b[39mvalue[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, gold]\n\u001b[1;32m    110\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m    111\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/nnsight/tracing/Proxy.py:41\u001b[0m, in \u001b[0;36mProxy.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mproxy \u001b[38;5;241m=\u001b[39m weakref\u001b[38;5;241m.\u001b[39mproxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalue\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     43\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Property to return the value of this proxy's node.\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m        Any: The stored value of the proxy, populated during execution of the model.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mdone():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "import yaml\n",
    "import os\n",
    "import json\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--config_file', type=str, help='path to the model training config file, found in broca/config')\n",
    "parser.add_argument('--stype', type=int, help='structure type idx. Can range from 0-30')\n",
    "\n",
    "args = parser.parse_args()\n",
    "with open(args.config_file, 'r') as f:\n",
    "    config_file = yaml.safe_load(f)\n",
    "\n",
    "# args = { \"config_file\": \"/mnt/align4_drive/arunas/broca/configs/mistral-atp-config\", \"stype\": 23 }\n",
    "# with open(args[\"config_file\"], 'r') as f:\n",
    "#    config_file = yaml.safe_load(f)\n",
    "\n",
    "print(json.dumps(config_file, indent=4))\n",
    "PREFIX = config_file[\"prefix\"]\n",
    "MODEL_NAME = config_file[\"model_name\"]\n",
    "MODEL_PATH = config_file[\"model_path\"]\n",
    "DATA_PATH = config_file[\"data_path\"]\n",
    "PROMPT_FILES_PATH = config_file[\"prompt_files_path\"]\n",
    "PATCH_PICKLES_PATH = config_file[\"patch_pickles_path\"]\n",
    "PATCH_PICKLES_SUBPATH = config_file[\"patch_pickles_sub_path\"]\n",
    "\n",
    "og = pd.read_csv(DATA_PATH)\n",
    "types = [col for col in og.columns if not 'ng-' in col]\n",
    "sType = types[args.stype]\n",
    "\n",
    "sType = types[0]\n",
    "\n",
    "if (not os.path.exists(f\"{PATCH_PICKLES_PATH}/attn/{PATCH_PICKLES_SUBPATH}/{sType}.pkl\") or not os.path.exists(f\"{PATCH_PICKLES_PATH}/mlp/{PATCH_PICKLES_SUBPATH}/{sType}.pkl\")):\n",
    "    print(f\"Running for {sType}\")\n",
    "    \n",
    "    if (MODEL_NAME == \"llama\"):\n",
    "        os.environ[\"HF_TOKEN\"] = config_file[\"hf_token\"]\n",
    "        nf4_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH, cache_dir=MODEL_CACHE_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\", cache_dir=MODEL_CACHE_PATH)\n",
    "        \n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = LanguageModel(MODEL_PATH,  quantization_config=nf4_config, tokenizer=tokenizer, device_map='auto', cache_dir=MODEL_CACHE_PATH) # Load the model\n",
    "\n",
    "    elif (MODEL_NAME == \"mistral\"):\n",
    "        config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, config=config, device_map=\"auto\", padding_side=\"left\")\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model = LanguageModel(MODEL_PATH,  tokenizer=tokenizer, device_map='auto') # Load the model\n",
    "        \n",
    "    model.requires_grad_(True)\n",
    "\n",
    "    def get_prompt_from_df(filename):\n",
    "        data = list(pd.read_csv(filename)['prompt'])\n",
    "        questions = list(pd.read_csv(filename)['q'])\n",
    "        golds = list(pd.read_csv(filename)['gold'])\n",
    "        return data, questions, golds\n",
    "\n",
    "    mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "    attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "\n",
    "    def attrPatching(cleanPrompt, q, gold, idx):\n",
    "        attn_layer_cache_prompt = {}\n",
    "        mlp_layer_cache_prompt = {}\n",
    "\n",
    "        attn_layer_cache_patch = {}\n",
    "        mlp_layer_cache_patch = {}\n",
    "\n",
    "        if gold == 'Yes':\n",
    "            testQ = q\n",
    "            patch = og[og[sType] == testQ][f\"ng-{sType}\"].head(1).item()\n",
    "            patchPrompt = cleanPrompt.replace(testQ, patch)\n",
    "        else:\n",
    "            patchPrompt = cleanPrompt\n",
    "            testQ = q\n",
    "            clean = og[og[f\"ng-{sType}\"] == testQ][sType].head(1).item()\n",
    "            cleanPrompt = patchPrompt.replace(testQ, clean)\n",
    "            gold = \"Yes\"\n",
    "\n",
    "        if model.tokenizer(cleanPrompt, return_tensors=\"pt\").input_ids.shape[-1] != \\\n",
    "            model.tokenizer(patchPrompt, return_tensors=\"pt\").input_ids.shape[-1]:\n",
    "            return\n",
    "\n",
    "        notGold = \"No\"\n",
    "        gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "        notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "        \n",
    "        with model.trace(cleanPrompt, scan=False, validate=False) as tracer:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "                mlp.retain_grad()\n",
    "                self_attn.retain_grad()\n",
    "\n",
    "                attn_layer_cache_prompt[layer] = {\"forward\": self_attn.save()} # \"backward\": self_attn.grad.detach().save()}\n",
    "                mlp_layer_cache_prompt[layer] = {\"forward\": mlp.save()}# \"backward\": mlp.grad.detach().save()}\n",
    "\n",
    "            logits = model.lm_head.output.save()\n",
    "        loss = logits.value[:, -1, notGold] - logits.value[:, -1, gold]\n",
    "        loss = loss.sum()\n",
    "        loss.backward()\n",
    "\n",
    "        with model.trace(patchPrompt, scan=False, validate=False) as tracer:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "\n",
    "                attn_layer_cache_patch[layer] = {\"forward\": self_attn.save()}\n",
    "                mlp_layer_cache_patch[layer] = {\"forward\": mlp.save()}\n",
    "\n",
    "        for layer in range(len(model.model.layers)):\n",
    "            mlp_effects = (mlp_layer_cache_prompt[layer][\"forward\"].value.grad * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "            attn_effects = (attn_layer_cache_prompt[layer][\"forward\"].value.grad * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)).detach()\n",
    "\n",
    "            mlp_effects = mlp_effects[0, -1, :] # batch, token, hidden_states\n",
    "            attn_effects = attn_effects[0, -1, :] # batch, token, hidden_states\n",
    "\n",
    "            mlp_effects_cache[layer] += mlp_effects.to(mlp_effects_cache[layer].get_device())\n",
    "            attn_effects_cache[layer] += attn_effects.to(mlp_effects_cache[layer].get_device())\n",
    "\n",
    "    prompts, questions, golds = get_prompt_from_df(f'{PROMPT_FILES_PATH}/{sType}.csv')\n",
    "    for idx,(prompt, q, gold) in tqdm(enumerate(zip(prompts,questions, golds))):\n",
    "        attrPatching(prompt, q, gold, idx)\n",
    "\n",
    "    mlp_effects_cache /= len(prompts)\n",
    "    attn_effects_cache /= len(prompts)\n",
    "\n",
    "    with open(f'{PATCH_PICKLES_PATH}/mlp/{PATCH_PICKLES_SUBPATH}/{sType}.pkl', 'wb') as f:\n",
    "        pickle.dump(mlp_effects_cache, f)\n",
    "\n",
    "    with open(f'{PATCH_PICKLES_PATH}/attn/{PATCH_PICKLES_SUBPATH}/{sType}.pkl', 'wb') as f:\n",
    "        pickle.dump(attn_effects_cache, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb704b1-27c0-4552-9c0d-8b5266314565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
