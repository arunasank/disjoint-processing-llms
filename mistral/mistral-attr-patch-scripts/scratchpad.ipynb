{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9c177fb-37a6-4d81-96fe-ed50746bfe42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/align4_drive/arunas/miniconda3/envs/broca/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoConfig\n",
    "import bitsandbytes\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = LanguageModel(\"mistralai/Mistral-7B-v0.1\",  quantization_config=nf4_config, dispatch=True, device_map='auto', cache_dir=\"/mnt/align4_drive/arunas/\") # Load the model\n",
    "\n",
    "og = pd.read_csv('/mnt/align4_drive/arunas/broca/data-gen/ngs.csv')\n",
    "og.columns\n",
    "\n",
    "def get_prompt_from_df(filename):\n",
    "    data = list(pd.read_csv(filename)['prompt'])\n",
    "    data = [sentence.strip() for sentence in data]\n",
    "    data = [sentence for sentence in data if not sentence == '']\n",
    "    data = [sentence.replace('</s>', '\\n') for sentence in data]\n",
    "    golds = [sentence.strip().split(\"\\n\")[-1].strip().split('A:')[-1].strip() for sentence in data]\n",
    "    data = [sentence[: -len(golds[idx])].strip() for idx, sentence in enumerate(data)]\n",
    "    return data, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df481100-ccb1-4b8b-b97a-191db8ad7c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "types = [col for col in list(og.columns) if not 'ng' in col]\n",
    "sType = types[0]\n",
    "\n",
    "prompts, golds = get_prompt_from_df(f'/mnt/align4_drive/arunas/broca/mistral/experiments/mistral-classification-train-test-det-{sType}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d6f5b6-4e5c-42c7-8078-1fd655d4b65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## For means later\n",
    "# mean = {}\n",
    "# nz_mean = {}\n",
    "# for i in range(len(types)):\n",
    "#     sType=types[i]\n",
    "#     with open(f'/mnt/align4_drive/arunas/broca/mistral/mistral-attr-patch-scripts/attn/new-{sType}-all-neurons.pkl', 'rb') as f:\n",
    "#         x = pickle.load(f)\n",
    "#         x = x.cpu()\n",
    "#         df = pd.DataFrame(x, index=range(32), columns=range(4096))\n",
    "#         mean = np.mean(df.to_numpy())\n",
    "#         if mean != 0:\n",
    "#             print(sType, np.mean([val for row in df.to_numpy() for val in row if val != 0]), mean)\n",
    "#         else:\n",
    "#             print(sType, mean)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "01e1cc18-d4ab-45d0-b938-b53ecb3342da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:00, 2835.91it/s]\n",
      "32it [00:00, 3263.57it/s]\n",
      "32it [00:00, 2918.92it/s]\n",
      "32it [00:00, 2828.32it/s]\n",
      "32it [00:00, 2982.15it/s]\n",
      "32it [00:00, 2749.29it/s]\n",
      "32it [00:00, 3182.25it/s]\n",
      "32it [00:00, 2847.15it/s]\n",
      "32it [00:00, 2813.26it/s]\n",
      "32it [00:00, 3214.49it/s]\n",
      "32it [00:00, 2754.31it/s]\n",
      "32it [00:00, 2826.95it/s]\n"
     ]
    }
   ],
   "source": [
    "attn_layer_activation_cache = {}\n",
    "mlp_layer_activation_cache = {}\n",
    "promptLogits = []\n",
    "shapes = []\n",
    "BATCH_SIZE = 32\n",
    "def pad_my_tensors(tensors):\n",
    "    devices = [t.device for t in tensors]\n",
    "    npArray1 = tensors[0].cpu()\n",
    "    npArray2 = tensors[1].cpu()\n",
    "    max_dim2 = max(npArray1.shape[1], npArray2.shape[1])\n",
    "    pad_width = [ (0,0),\n",
    "                  (0, max_dim2 - npArray1.shape[1]),\n",
    "                  (0,0)\n",
    "                ]\n",
    "    npArray1 = np.pad(npArray1, pad_width, mode='constant', constant_values=0)\n",
    "    pad_width = [ (0,0),\n",
    "                  (0, max_dim2 - npArray2.shape[1]),\n",
    "                  (0,0)\n",
    "                ]\n",
    "    npArray2 = np.pad(npArray2, pad_width, mode='constant', constant_values=0)\n",
    "    return [torch.tensor(npArray1).to(devices[0]), torch.tensor(npArray2).to(devices[1])]\n",
    "\n",
    "for i in range(0, len(prompts) // BATCH_SIZE):\n",
    "    start = (i*BATCH_SIZE)\n",
    "    end = min((i+1)*BATCH_SIZE, len(prompts))\n",
    "    promptSet = []\n",
    "    gs = []\n",
    "    nGs = []\n",
    "    logitDiffs = []\n",
    "    for prompt,gold in tqdm(zip(prompts[start:end], golds[start:end])):\n",
    "        try:\n",
    "            if (gold == 'Yes'):\n",
    "                predictionExample = prompt[prompt[:-2].rfind(':')+1:-2].strip()\n",
    "                patch = og[og[sType] == predictionExample][f\"ng-{sType}\"].iloc[0]\n",
    "                patchPrompt = prompt.replace(predictionExample, patch)\n",
    "            else:\n",
    "                patchPrompt = prompt\n",
    "                patch = prompt[prompt[:-2].rfind(':')+1:-2].strip()\n",
    "                predictionExample = og[og[f\"ng-{sType}\"] == patch][sType].iloc[0]\n",
    "                prompt = patchPrompt.replace(patch, predictionExample)\n",
    "                gold = \"Yes\"\n",
    "    \n",
    "            notGold = \"No\"\n",
    "            promptSet.append(prompt)\n",
    "            gold = model.tokenizer(gold)['input_ids']\n",
    "            notGold = model.tokenizer(notGold)['input_ids']\n",
    "            gs.append(gold)\n",
    "            nGs.append(notGold)\n",
    "        except:\n",
    "            print(f\"Error with stype: {sType} prompt: {prompt} gold: {gold}\", traceback.format_exc())\n",
    "            continue\n",
    "    # print(len(promptSet), promptSet[0])\n",
    "    with model.trace(promptSet, scan=False, validate=False) as tracer: \n",
    "        for layer in range(len(model.model.layers)):\n",
    "            self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "            mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "\n",
    "            if (not layer in attn_layer_activation_cache):\n",
    "                # print(self_attn.detach().save().shape, mlp.detach().save().shape)\n",
    "                attn_layer_activation_cache[layer] = [self_attn.detach().save()]\n",
    "                mlp_layer_activation_cache[layer] = [mlp.detach().save()]\n",
    "            else:\n",
    "                attn_layer_activation_cache[layer].append(self_attn.detach().save())\n",
    "                mlp_layer_activation_cache[layer].append(mlp.detach().save())\n",
    "        logits = (model.lm_head.output[:, -1, notGold] - model.lm_head.output[:, -1, gold]).detach().save()\n",
    "        logitDiffs.append(logits)\n",
    "        # shapes.append((model.lm_head.output[:, -1, notGold].detach().save(), model.lm_head.output[:, -1, gold].detach().save()))\n",
    "    promptLogits.append(torch.cat([p[:, -1] for p in logitDiffs]))\n",
    "    if (i > 0):\n",
    "        for layer in range(len(model.model.layers)):\n",
    "            attn_layer_activation_cache[layer] = pad_my_tensors(attn_layer_activation_cache[layer])\n",
    "            attn_layer_activation_cache[layer] = [torch.sum(torch.stack(attn_layer_activation_cache[layer]), dim=0)]\n",
    "            mlp_layer_activation_cache[layer] = pad_my_tensors(mlp_layer_activation_cache[layer])\n",
    "            mlp_layer_activation_cache[layer] = [torch.sum(torch.stack(mlp_layer_activation_cache[layer]), dim=0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "28b7730b-8ff6-4535-991e-c30fbdc67dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_attn_activation = torch.sum(torch.stack([attn_layer_activation_cache[l][0].detach().cpu().sum() for l in range(len(model.model.layers))]), dtype=torch.float64) / len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5a8a1987-b718-41fa-9099-20ab60b6d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_mlp_activation = torch.sum(torch.stack([mlp_layer_activation_cache[l][0].detach().cpu().sum() for l in range(len(model.model.layers))]), dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "39032b79-d6a6-45ca-9e04-b3f5eeb4e101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(-12936., dtype=torch.float16),\n",
       " tensor(-inf, dtype=torch.float16),\n",
       " tensor(15280., dtype=torch.float16),\n",
       " tensor(13120., dtype=torch.float16),\n",
       " tensor(22896., dtype=torch.float16),\n",
       " tensor(6748., dtype=torch.float16),\n",
       " tensor(29952., dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(-19328., dtype=torch.float16),\n",
       " tensor(62976., dtype=torch.float16),\n",
       " tensor(-10816., dtype=torch.float16),\n",
       " tensor(-30256., dtype=torch.float16),\n",
       " tensor(48384., dtype=torch.float16),\n",
       " tensor(-53312., dtype=torch.float16),\n",
       " tensor(-1921., dtype=torch.float16),\n",
       " tensor(35488., dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(43776., dtype=torch.float16),\n",
       " tensor(27488., dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(-inf, dtype=torch.float16),\n",
       " tensor(-inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(-20032., dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(inf, dtype=torch.float16),\n",
       " tensor(-inf, dtype=torch.float16)]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[mlp_layer_activation_cache[l][0].detach().cpu().sum() for l in range(len(model.model.layers))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cacc7c44-bc13-4fa6-8c13-1632b658d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "promptLogits = torch.cat(promptLogits)\n",
    "for layer in range(len(model.model.layers)):\n",
    "    attn_layer_activation_cache[layer] = attn_layer_activation_cache[layer][0]\n",
    "    attn_layer_activation_cache[layer] = torch.sum(attn_layer_activation_cache[layer], dim=0)\n",
    "    mlp_layer_activation_cache[layer] = mlp_layer_activation_cache[layer][0]\n",
    "    mlp_layer_activation_cache[layer] = torch.sum(mlp_layer_activation_cache[layer], dim=0)\n",
    "    \n",
    "\n",
    "overall_attn_activation = sum([attn_layer_activation_cache[layer].detach().cpu() for layer in range(len(model.model.layers))])\n",
    "overall_mlp_activation = sum([mlp_layer_activation_cache[layer].detach().cpu() for layer in range(len(model.model.layers))])\n",
    "    \n",
    "overall_attn_activation /= len(prompts)\n",
    "overall_mlp_activation /= len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cdb81e46-aa9c-46c1-92a5-64277baca438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([289, 4096]), torch.Size([289, 4096]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_attn_activation.shape, overall_mlp_activation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83798c9-4c6a-416d-9c2c-b7637e7b963f",
   "metadata": {},
   "outputs": [],
   "source": [
    "* You have an average activation. \n",
    "* You apply this neuron by neuron.\n",
    "    You run through your dataset of prompts, gather all the difference of the logit diffs and store the average difference of logit diffs pertaining to this neuron.\n",
    "    Then you get the top 40 neurons based on this average.\n",
    "* You get the logit diff, and compare it with the original logit diff and store the neuron if it's in the top 1% of neurons causing this diff."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
