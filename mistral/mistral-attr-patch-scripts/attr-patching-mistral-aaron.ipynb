{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a900da82-e783-4757-b886-8c82bcb90924",
   "metadata": {},
   "source": [
    "## Mistral Attribution Patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47a6e939-c1ca-44b6-a6a3-d3a0a35c77ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e608719b-53be-4dc8-b76e-d6a85812b7d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abb8dbf78cd94412b4ce66059b9aa202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device='cuda:0'\n",
    "model = LanguageModel(\"/home/gridsan/arunas/models/mistralai/Mistral-7B-v0.1/\",  load_in_8bit=True, dispatch=True, device_map=device) # Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd983e9-557f-4589-830a-cc15d5b738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "og = pd.read_csv('ngs.csv')\n",
    "og.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837b3d1c-abdc-4fa6-8056-b6f674cb2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_from_text(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = file.read().split('\\n-------------------------------------\\n')\n",
    "    \n",
    "    data = [sentence.strip() for sentence in data]\n",
    "    data = [sentence for sentence in data if not sentence == '']\n",
    "    data = [sentence.replace('</s>', '\\n') for sentence in data]\n",
    "    golds = [sentence.strip().split(\"\\n\")[-1].strip() for sentence in data]\n",
    "    data = [sentence[: -len(golds[idx])].strip() for idx, sentence in enumerate(data)]\n",
    "    data = [sentence[: -len(sentence.strip().split(\" \")[-1])].strip() for sentence in data]\n",
    "    return data, golds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c397f83f-4d9b-41e5-bb25-095bc8bc4c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aeca53-a1db-4a37-802b-8a370db40233",
   "metadata": {},
   "outputs": [],
   "source": [
    "sType=sys.argv[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b90b2b-200d-43c7-a493-1a673b70153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "attn_effects_cache = torch.zeros((model.config.num_hidden_layers, model.config.hidden_size)).to(\"cuda\")\n",
    "\n",
    "def attrPatching(fullPrompt, gold):\n",
    "    attn_layer_cache_prompt = {}\n",
    "    mlp_layer_cache_prompt = {}\n",
    "    \n",
    "    attn_layer_cache_patch = {}\n",
    "    mlp_layer_cache_patch = {}\n",
    "    if (gold == 'Yes'):\n",
    "        predictionExample = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        patch = og[og[sType] == predictionExample][f\"ng-{sType}\"].iloc[0]\n",
    "        patchPrompt = fullPrompt.replace(predictionExample, patch)\n",
    "    else:\n",
    "        patchPrompt = fullPrompt\n",
    "        patch = fullPrompt[fullPrompt[:-2].rfind(':')+1:-2].strip()\n",
    "        predictionExample = og[og[f\"ng-{sType}\"] == patch][sType].iloc[0]\n",
    "        fullPrompt = patchPrompt.replace(patch, predictionExample)\n",
    "        gold = \"Yes\"\n",
    "\n",
    "    notGold = \"No\"\n",
    "    gold = model.tokenizer(gold)[\"input_ids\"]\n",
    "    notGold = model.tokenizer(notGold)[\"input_ids\"]\n",
    "    with model.forward(inference=False) as runner:\n",
    "        with runner.invoke(fullPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "    \n",
    "                attn_layer_cache_prompt[layer] = {\"forward\": self_attn.detach().save(), \"backward\": self_attn.grad.detach().save()}\n",
    "                mlp_layer_cache_prompt[layer] = {\"forward\": mlp.detach().save(), \"backward\": mlp.grad.detach().save()}\n",
    "            \n",
    "            logits = model.lm_head.output[:, -1, notGold] - model.lm_head.output[:, -1, gold]\n",
    "            loss = logits.sum()\n",
    "            loss.backward(retain_graph=False)\n",
    "    \n",
    "    with model.forward(inference=False) as runner:\n",
    "        with runner.invoke(patchPrompt) as invoker:\n",
    "            for layer in range(len(model.model.layers)):\n",
    "                self_attn = model.model.layers[layer].self_attn.o_proj.output\n",
    "                mlp = model.model.layers[layer].mlp.down_proj.output\n",
    "                \n",
    "                attn_layer_cache_patch[layer] = {\"forward\": self_attn.detach().save()}\n",
    "                mlp_layer_cache_patch[layer] = {\"forward\": mlp.detach().save()}\n",
    "    \n",
    "    for layer in range(len(model.model.layers)):\n",
    "        mlp_effects = mlp_layer_cache_prompt[layer][\"backward\"].value * (mlp_layer_cache_patch[layer][\"forward\"].value - mlp_layer_cache_prompt[layer][\"forward\"].value)\n",
    "        attn_effects = attn_layer_cache_prompt[layer][\"backward\"].value * (attn_layer_cache_patch[layer][\"forward\"].value - attn_layer_cache_prompt[layer][\"forward\"].value)\n",
    "\n",
    "        mlp_effects = mlp_effects[:, -1, :] # batch, token, hidden_states\n",
    "        attn_effects = attn_effects[:, -1, :] # batch, token, hidden_states\n",
    "\n",
    "        mlp_effects_cache[layer] += mlp_effects[0]\n",
    "        attn_effects_cache[layer] += attn_effects[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f45bfe4-996e-4a39-a421-2adb2e824a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts, golds = get_prompt_from_text(f'classification-train-test-{sType}-prompts.txt')\n",
    "for prompt,gold in tqdm(zip(prompts, golds)):\n",
    "    attrPatching(prompt, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8764bd4b-3d62-42f0-96de-d05d71dfc2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 4096])\n"
     ]
    }
   ],
   "source": [
    "mlp_effects_cache /= len(prompts)\n",
    "attn_effects_cache /= len(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8879b0c2-9445-40e9-be46-302661bf8068",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_effects_cache = torch.nan_to_num(mlp_effects_cache)\n",
    "attn_effects_cache = torch.nan_to_num(attn_effects_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f18a29c1-de03-48b2-8f93-7d07911e43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_effects_cache = mlp_effects_cache.view(-1)\n",
    "top_neurons = flattened_effects_cache.topk(k=40)\n",
    "two_d_indices = torch.cat((((top_neurons[1] // mlp_effects_cache.shape[1]).unsqueeze(1)), ((top_neurons[1] % mlp_effects_cache.shape[1]).unsqueeze(1))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39b1721-811b-4025-a9a9-23cfb6ac8982",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'mlp/{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(two_d_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "abc286bc-4cd3-4bad-b302-37a5f072305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_effects_cache = attn_effects_cache.view(-1)\n",
    "top_neurons = flattened_effects_cache.topk(k=40)\n",
    "two_d_indices = torch.cat((((top_neurons[1] // attn_effects_cache.shape[1]).unsqueeze(1)), ((top_neurons[1] % attn_effects_cache.shape[1]).unsqueeze(1))), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "96792c01-37cf-484d-b597-b98ccaa127f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  31, 2936],\n",
       "        [  31, 2216],\n",
       "        [  31, 2078],\n",
       "        [  31,  685],\n",
       "        [  31,  581],\n",
       "        [  31,  502],\n",
       "        [  31, 2379],\n",
       "        [  31, 2812],\n",
       "        [  31, 1878],\n",
       "        [  31,  239],\n",
       "        [  31, 3025],\n",
       "        [  31,  273],\n",
       "        [  31, 3833],\n",
       "        [  31, 1519],\n",
       "        [  31, 1887],\n",
       "        [  31, 1720],\n",
       "        [  31, 1453],\n",
       "        [  31, 1416],\n",
       "        [  31, 3595],\n",
       "        [  31, 2899],\n",
       "        [  31,  979],\n",
       "        [  31, 3707],\n",
       "        [  31, 1603],\n",
       "        [  31, 3142],\n",
       "        [  31, 2215],\n",
       "        [  31,   65],\n",
       "        [  31, 2876],\n",
       "        [  31,  345],\n",
       "        [  31, 2009],\n",
       "        [  31, 1336],\n",
       "        [  31, 3661],\n",
       "        [  31, 3519],\n",
       "        [  31, 2454],\n",
       "        [  31, 2141],\n",
       "        [  31, 2015],\n",
       "        [  31, 1145],\n",
       "        [  31, 2618],\n",
       "        [  31, 1957],\n",
       "        [  31, 3832],\n",
       "        [  31, 2296]], device='cuda:0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'attn/{sType}.pkl', 'wb') as f:\n",
    "    pickle.dump(two_d_indices, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b4e249-f775-413f-ab66-4f3a90d9fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
