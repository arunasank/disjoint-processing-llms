0 30
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [00:01<00:08,  1.72s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [00:03<00:06,  1.69s/it]Loading checkpoint shards:  50%|█████     | 3/6 [00:05<00:05,  1.70s/it]Loading checkpoint shards:  67%|██████▋   | 4/6 [00:06<00:03,  1.69s/it]Loading checkpoint shards:  83%|████████▎ | 5/6 [00:08<00:01,  1.67s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:09<00:00,  1.59s/it]Loading checkpoint shards: 100%|██████████| 6/6 [00:09<00:00,  1.64s/it]
Layer: 30
/home/gridsan/arunas/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:226: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.
  warnings.warn(f'Input type into Linear4bit is torch.float16, but bnb_4bit_compute_type=torch.float32 (default). This will lead to slow inference or training speed.')
Traceback (most recent call last):
  File "/home/gridsan/arunas/broca/mistral-activations/l-script.py", line 50, in <module>
    curr_activation = get_activation(row, layer)
  File "/home/gridsan/arunas/broca/mistral-activations/l-script.py", line 35, in get_activation
    outputs = model(**tokens, output_hidden_states=True, output_attentions=True)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 1009, in forward
    outputs = self.model(
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 897, in forward
    layer_outputs = decoder_layer(
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 639, in forward
    hidden_states = self.mlp(hidden_states)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py", line 175, in forward
    return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1518, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1527, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/accelerate/hooks.py", line 164, in new_forward
    output = module._old_forward(*args, **kwargs)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/bitsandbytes/nn/modules.py", line 256, in forward
    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 577, in matmul_4bit
    return MatMul4Bit.apply(A, B, out, bias, quant_state)
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/torch/autograd/function.py", line 539, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/home/gridsan/arunas/.local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py", line 516, in forward
    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 1 has a total capacty of 31.74 GiB of which 224.88 MiB is free. Including non-PyTorch memory, this process has 31.52 GiB memory in use. Of the allocated memory 30.11 GiB is allocated by PyTorch, and 525.03 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
